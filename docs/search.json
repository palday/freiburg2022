[
  {
    "objectID": "02-simulation-in-R.html",
    "href": "02-simulation-in-R.html",
    "title": "Simulation in R",
    "section": "",
    "text": "\\[Y \\sim \\mathcal{N}(\\mu,\\sigma^2)\\] \\[\\mu = X\\beta \\]\n\ntildes/distributions correspond to the r* family of functions (random draws)\neverything else is a little bit of matrix math\n\n\n\n\n\nlibrary(\"groundhog\")\n\nLoaded 'groundhog' (version:2.0.1) using R-4.2.0\n\n\nTips and troubleshooting: https://groundhogR.com\n\nset.seed(42)\nform <- y ~ 1 + x\nbeta <- c(1, 42)\nsig <- 3.14\nn <- 100\n\n# we can start with mean zero and then add mu in later\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=runif(n, -10, 10)  # NB: uniform\n                  )\n\nX <- model.matrix(form, dat)\n\ndat$y <- X %*% beta + dat$y\nsummary(lm(form, dat))\n\n\nCall:\nlm(formula = form, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.6190 -2.0023  0.4292  2.1287  6.8879 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.15594    0.33461   3.455 0.000816 ***\nx           42.04546    0.05749 731.355  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.276 on 98 degrees of freedom\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9998 \nF-statistic: 5.349e+05 on 1 and 98 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nset.seed(42)\nform <- y ~ 1 + x * z\n# intercept, x, z, x:z\nbeta <- c(1, 42, 27, 0.4)\nsig <- 3.14\nn <- 100\n\n# we can start with mean zero and then add mu in later\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=runif(n, -10, 10),  # NB: uniform\n                  z=runif(n, 25, 75))\n\nX <- model.matrix(form, dat)\n\ndat$y <- X %*% beta + dat$y\nm2 <- lm(form, dat)\nsummary(m2)\n\n\nCall:\nlm(formula = form, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -1.8976  0.5459  2.2200  7.0766 \n\nCoefficients:\n             Estimate Std. Error  t value Pr(>|t|)    \n(Intercept)  1.975166   1.202758    1.642    0.104    \nx           41.869442   0.218333  191.768   <2e-16 ***\nz           26.983065   0.022726 1187.342   <2e-16 ***\nx:z          0.403483   0.004046   99.728   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.283 on 96 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 9.614e+05 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nset.seed(42)\nform <- y ~ 1 + x * z\n# intercept, x, z, x:z\nbeta <- c(1, 42, 27, 0.4)\nsig <- 3.14\nn <- 100\n\n# we can start with mean zero and then add mu in later\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=runif(n, -10, 10),  # NB: uniform\n                  z=runif(n, 25, 75))\n\nX <- model.matrix(form, dat)\n\ndat$y <- X %*% beta + dat$y\nm2c <- lm(y ~ 1 + x * scale(z, center=TRUE, scale=FALSE), dat)\nsummary(m2c)\n\n\nCall:\nlm(formula = y ~ 1 + x * scale(z, center = TRUE, scale = FALSE), \n    data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -1.8976  0.5459  2.2200  7.0766 \n\nCoefficients:\n                                          Estimate Std. Error t value Pr(>|t|)\n(Intercept)                              1.355e+03  3.380e-01 4009.82   <2e-16\nx                                        6.211e+01  5.838e-02 1063.82   <2e-16\nscale(z, center = TRUE, scale = FALSE)   2.698e+01  2.273e-02 1187.34   <2e-16\nx:scale(z, center = TRUE, scale = FALSE) 4.035e-01  4.046e-03   99.73   <2e-16\n                                            \n(Intercept)                              ***\nx                                        ***\nscale(z, center = TRUE, scale = FALSE)   ***\nx:scale(z, center = TRUE, scale = FALSE) ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.283 on 96 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 9.614e+05 on 3 and 96 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\ngroundhog.library(\"effects\", \"2022-07-14\")\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\n\n\u001b[36mSuccesfully attached 'effects_4.2-2'\u001b[0m\n\nplot(allEffects(m2), multiline=TRUE, ci.style=\"band\")\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictor y is a one-column matrix that was converted to a vector\n\n\n\n\n\n\n\n\n\nplot(allEffects(m2c), multiline=TRUE, ci.style=\"band\")\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictors y, scale(z, center = TRUE, scale = FALSE) are one-column matrices\nthat were converted to vectors\n\n\n\n\n\n\n\n\n\nLinear transformations of the numeric representation of your predictors changes the meaning and interpretation of your model coefficients\nLikewise, the way that categorical variables are changed into contrasts determines the meaning and interpretation of your model coefficients\nYou cannot understand what a model means without knowing the choice of contrast and any transformation of numeric predictors! (see also Brehm and Alday, JML, in press)\nThis is just as critical for simulation, if you want to simulate your hypotheses!\n\n\n\n\nusing DataFrames, GLM, StatsModels, Random\n\nrng = MersenneTwister(42);\nform = @formula(y ~ 1 + x * z);\n# intercept, x, z, x&z\nbeta = [1, 42, 27, 0.4];\nsig = 3.14;\nn = 100;\n\n# we can start with mean zero and then add mu in later\ndat = DataFrame(y=sig * randn(rng, n), # multiply by sd to scale\n                x=20 * rand(rng, n) .- 10,  # multiply by range and move left edge to correct postion\n                z=50 * rand(rng, n) .+ 25);\n\n# \"hints\" is the argument contrasts, etc.\nX = modelmatrix(form, dat);\n\n\nm2 = lm(form, dat)"
  },
  {
    "objectID": "02-simulation-in-R.html#imbalance",
    "href": "02-simulation-in-R.html#imbalance",
    "title": "Simulation in R",
    "section": "Imbalance",
    "text": "Imbalance\n\ngroundhog.library(\"car\", \"2022-07-14\")\n\n\u001b[36mSuccesfully attached 'car_3.1-0'\u001b[0m\n\nset.seed(42)\nform <- y ~ 1 + x\nbeta <- c(1, 42, 42)\nsig <- 3.14\nn <- 100\n\n# we can start with mean zero and then add mu in later\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=sample(c(\"brown\", \"green\", \"orange\"), n, replace=TRUE)  # NB: uniform\n                  )\ndat$x <- factor(dat$x)\ncontrasts(dat$x) <- contr.Sum(levels(dat$x))\n\nX <- model.matrix(form, dat)\n\ndat$y <- X %*% beta + dat$y\nsummary(lm(form, dat))\n\n\nCall:\nlm(formula = form, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3230 -2.0847  0.3634  2.0419  6.6855 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.0739     0.3305    3.25  0.00159 ** \nx[S.brown]   42.4207     0.4517   93.91  < 2e-16 ***\nx[S.green]   41.7285     0.4710   88.60  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.288 on 97 degrees of freedom\nMultiple R-squared:  0.9969,    Adjusted R-squared:  0.9968 \nF-statistic: 1.548e+04 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\n\nset.seed(42)\nform <- y ~ 1 + x\nbeta <- c(1, 42, 42)\nsig <- 3.14\nn <- 100\n\n# we can start with mean zero and then add mu in later\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=sample(c(\"brown\", \"green\", \"orange\"), n, replace=TRUE, prob=c(19/30, 1/30, 1/3))  # NB: uniform\n                  )\ndat$x <- factor(dat$x)\ncontrasts(dat$x) <- contr.Sum(levels(dat$x))\n\nX <- model.matrix(form, dat)\n\ndat$y <- X %*% beta + dat$y\nsummary(lm(form, dat))\n\n\nCall:\nlm(formula = form, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.1533 -2.2243  0.2046  2.2696  6.5829 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.9428     0.6780   2.866   0.0051 ** \nx[S.brown]   40.8121     0.7111  57.390   <2e-16 ***\nx[S.green]   43.0412     1.2752  33.753   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.24 on 97 degrees of freedom\nMultiple R-squared:  0.9961,    Adjusted R-squared:  0.996 \nF-statistic: 1.23e+04 on 2 and 97 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "02-simulation-in-R.html#heavy-tails",
    "href": "02-simulation-in-R.html#heavy-tails",
    "title": "Simulation in R",
    "section": "Heavy Tails",
    "text": "Heavy Tails\n\nset.seed(42)\nform <- y ~ 1 + x\nbeta <- c(1, 42)\nsig <- 3.14\nn <- 100\n\n# we can start with mean zero and then add mu in later\ndat <- data.frame(y=sig * rt(n, df=3),\n                  x=runif(n, -10, 10)  # NB: uniform\n                  )\n\nX <- model.matrix(form, dat)\n\ndat$y <- X %*% beta + dat$y\nsummary(lm(form, dat))\n\n\nCall:\nlm(formula = form, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4307 -2.6328 -0.8111  1.4640 29.5658 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.23958    0.48927   2.534   0.0129 *  \nx           41.83358    0.08632 484.656   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.89 on 98 degrees of freedom\nMultiple R-squared:  0.9996,    Adjusted R-squared:  0.9996 \nF-statistic: 2.349e+05 on 1 and 98 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "02-simulation-in-R.html#outliers",
    "href": "02-simulation-in-R.html#outliers",
    "title": "Simulation in R",
    "section": "Outliers",
    "text": "Outliers\n\nset.seed(42)\nform <- y ~ 1 + x\nbeta <- c(1, 42)\nsig <- 3.14\nn <- 100\n\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=runif(n, -10, 10)  # NB: uniform\n                  )\n\nX <- model.matrix(form, dat)\n# in 10% of data points, we add in a potentially huge second source of error\ndat$y <- dat$y + rbinom(n, 1, prob=0.1) * rnorm(n, sd=100*sig)\n\ndat$y <- X %*% beta + dat$y\nsummary(lm(form, dat))\n\n\nCall:\nlm(formula = form, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-541.35  -13.20    0.63   13.58  664.77 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -8.038     12.774  -0.629    0.531    \nx             39.016      2.195  17.778   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 125.1 on 98 degrees of freedom\nMultiple R-squared:  0.7633,    Adjusted R-squared:  0.7609 \nF-statistic:   316 on 1 and 98 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "02-simulation-in-R.html#recall.",
    "href": "02-simulation-in-R.html#recall.",
    "title": "Simulation in R",
    "section": "Recall….",
    "text": "Recall….\n\\[\\begin{align*}\n(Y | B = b ) &\\sim \\mathcal{N}( X\\beta + Zb, \\sigma^2 I ) \\\\\n  B &\\sim \\mathcal{N}(0, \\sigma _\\theta)\n\\end{align*}\\]\n\ntildes/distributions correspond to the r* family of functions (random draws)\neverything else is a little bit of matrix math"
  },
  {
    "objectID": "02-simulation-in-R.html#a-basic-example",
    "href": "02-simulation-in-R.html#a-basic-example",
    "title": "Simulation in R",
    "section": "A basic example",
    "text": "A basic example\n\nset.seed(42)\ngroundhog.library(\"lme4\", \"2022-07-14\")\n\nLoading required package: Matrix\n\n\n\u001b[36mSuccesfully attached 'lme4_1.1-30'\u001b[0m\n\nform_fe <- y ~ 1 + x\nform <- y ~ 1 + x + (1 | subj)\n\nbeta <- c(1, 42)\nsig <- 1 # try 3.14\nn <- 500 # try 100, 200, 500, 1000\nn_subj <- 20 # try 10, 15, can't be more than 26\n# was it better to increase n or n_subj?\nsubj_sd <- 0.5\nb <- rnorm(n_subj, sd=subj_sd)\n\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=runif(n, -10, 10),\n                  subj=sample(LETTERS[1:n_subj], n, replace=TRUE))\n\nX <- model.matrix(form_fe, dat)\nZ <- model.matrix(~ 0 + subj, dat)\n\ndat$y <- X %*% beta + Z %*% b + dat$y\n# what happens if you use y ~ 1 + x + (1 + x | subj)?\nmm1 <- lmer(form, dat, REML=FALSE)\nsummary(mm1)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  1440.1   1456.9   -716.0   1432.1      496 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.07161 -0.67603 -0.00644  0.66172  3.04554 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subj     (Intercept) 0.4708   0.6862  \n Residual             0.9253   0.9619  \nNumber of obs: 500, groups:  subj, 20\n\nFixed effects:\n             Estimate Std. Error  t value\n(Intercept)  1.051725   0.159618    6.589\nx           42.011144   0.007593 5532.653\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.006"
  },
  {
    "objectID": "02-simulation-in-R.html#check-to-make-sure-that-we-recover-the-blups",
    "href": "02-simulation-in-R.html#check-to-make-sure-that-we-recover-the-blups",
    "title": "Simulation in R",
    "section": "Check to make sure that we recover the BLUPs",
    "text": "Check to make sure that we recover the BLUPs\n\ngroundhog.library(c(\"broom.mixed\",\"ggplot2\"), \"2022-07-14\")\n\n\u001b[36mSuccesfully attached 'broom.mixed_0.2.9.4'\u001b[0m\n\n\n\u001b[36mSuccesfully attached 'ggplot2_3.3.6'\u001b[0m\n\ndd <- tidy(mm1, effects=\"ran_vals\")\ndd <- transform(dd, level=reorder(level,estimate))\ntruth <- data.frame(level=LETTERS[1:n_subj],estimate=b)\nggplot(dd,aes(x=level,y=estimate))+\n    geom_pointrange(aes(ymin=estimate-2*std.error,\n                        ymax=estimate+2*std.error)) + coord_flip() +\n    geom_point(data=truth, colour=\"red\") +\n    theme_light()"
  },
  {
    "objectID": "02-simulation-in-R.html#complexity-explodes-when-doing-this-by-hand",
    "href": "02-simulation-in-R.html#complexity-explodes-when-doing-this-by-hand",
    "title": "Simulation in R",
    "section": "Complexity explodes when doing this by hand",
    "text": "Complexity explodes when doing this by hand\n\nset.seed(42)\nform_fe <- y ~ 1 + x\nform <- y ~ 1 + x + (1 + x| subj)\n\nbeta <- c(1, 42)\nsig <- 1.414\nn <- 1000\nn_subj <- 26 # can't be more than 26 ;)\nsubj_sd <- c(0.5, 0.3)\n# for now, we just let the RE correlations be zero\nb <- c(rnorm(n_subj, sd=subj_sd[1]), rnorm(n_subj, sd=subj_sd[2]))\n\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=runif(n, -10, 10),\n                  subj=rep_len(LETTERS[1:n_subj], n))\n\nX <- model.matrix(form_fe, dat)\nZ_int <- model.matrix(~ 0 + subj, dat)\nZ_slope <- ... # I can't be bothered ot actually do write this out....\nZ <- cbind(Z_int, Z_slope)\n\ndat$y <- X %*% beta + Z %*% b + dat$y\nmm2 <- lmer(form, dat, REML=FALSE)\nsummary(mm1)"
  },
  {
    "objectID": "02-simulation-in-R.html#let-lme4-do-the-hard-work-for-you",
    "href": "02-simulation-in-R.html#let-lme4-do-the-hard-work-for-you",
    "title": "Simulation in R",
    "section": "Let lme4 do the hard work for you",
    "text": "Let lme4 do the hard work for you\n\nset.seed(42)\nform <- y ~ 1 + x + (1 + x| subj)\n\nbeta <- c(1, 42)\nsig <- 1.414\nn <- 1000\nn_subj <- 26 # can't be more than 26 ;)\n# for now, we just let the RE correlations be zero\nsubj_sd <- c(0.5, 0.3)\n\ndat <- data.frame(y=rnorm(n, sd=sig),\n                  x=runif(n, -10, 10),\n                  subj=rep_len(LETTERS[1:n_subj], n))\nmm2 <- lmer(form, dat, REML=FALSE)\n\nboundary (singular) fit: see help('isSingular')\n\n# this is garbage, but that's fine!\nsummary(mm2)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 + x | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  3545.3   3574.8  -1766.7   3533.3      994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1872 -0.6613 -0.0058  0.6801  3.5238 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n subj     (Intercept) 0.0122816 0.11082      \n          x           0.0001533 0.01238  1.00\n Residual             1.9893385 1.41044      \nNumber of obs: 1000, groups:  subj, 26\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) -0.034332   0.049646  -0.692\nx           -0.002603   0.008101  -0.321\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.117 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "02-simulation-in-R.html#simr-fixef-i",
    "href": "02-simulation-in-R.html#simr-fixef-i",
    "title": "Simulation in R",
    "section": "simr fixef I",
    "text": "simr fixef I\n\ngroundhog.library(\"simr\", \"2022-07-14\")\n\n\nAttaching package: 'simr'\n\n\nThe following object is masked from 'package:lme4':\n\n    getData\n\n\n\u001b[36mSuccesfully attached 'simr_1.0.6'\u001b[0m\n\nfixef(mm2) <- beta\n# note that the estimates have been overridden, but nothing else updated\nsummary(mm2)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 + x | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  3545.3   3574.8  -1766.7   3533.3      994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1872 -0.6613 -0.0058  0.6801  3.5238 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n subj     (Intercept) 0.0122816 0.11082      \n          x           0.0001533 0.01238  1.00\n Residual             1.9893385 1.41044      \nNumber of obs: 1000, groups:  subj, 26\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  1.000000   0.049646   20.14\nx           42.000000   0.008101 5184.58\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.117 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "02-simulation-in-R.html#simr-fixef-ii",
    "href": "02-simulation-in-R.html#simr-fixef-ii",
    "title": "Simulation in R",
    "section": "simr fixef II",
    "text": "simr fixef II\n\nsummary(refit(mm2, simulate(mm2)[,1]))\n\nboundary (singular) fit: see help('isSingular')\n\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 + x | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  3546.4   3575.9  -1767.2   3534.4      994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.8279 -0.6810 -0.0193  0.6511  3.6094 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr \n subj     (Intercept) 0.0143537 0.119807      \n          x           0.0000646 0.008037 -1.00\n Residual             1.9925556 1.411579      \nNumber of obs: 1000, groups:  subj, 26\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  1.030736   0.050464   20.43\nx           41.999333   0.007885 5326.45\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.111\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "02-simulation-in-R.html#simr-sig",
    "href": "02-simulation-in-R.html#simr-sig",
    "title": "Simulation in R",
    "section": "simr sig",
    "text": "simr sig\n\nsigma(mm2) <- sig\nsummary(mm2)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 + x | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  3545.3   3574.8  -1766.7   3533.3      994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1792 -0.6596 -0.0057  0.6784  3.5150 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n subj     (Intercept) 0.0122816 0.11082      \n          x           0.0001533 0.01238  1.00\n Residual             1.9993960 1.41400      \nNumber of obs: 1000, groups:  subj, 26\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  1.000000   0.049771   20.09\nx           42.000000   0.008121 5171.53\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.117 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "02-simulation-in-R.html#varcorr-the-variance-covariance-of-the-random-effects",
    "href": "02-simulation-in-R.html#varcorr-the-variance-covariance-of-the-random-effects",
    "title": "Simulation in R",
    "section": "VarCorr: the variance-covariance of the random effects",
    "text": "VarCorr: the variance-covariance of the random effects\n\nvc <- VarCorr(mm2)\nprint(vc)\n\n Groups   Name        Std.Dev. Corr \n subj     (Intercept) 0.110822      \n          x           0.012383 1.000\n Residual             1.414000"
  },
  {
    "objectID": "02-simulation-in-R.html#simr-random-effects-i",
    "href": "02-simulation-in-R.html#simr-random-effects-i",
    "title": "Simulation in R",
    "section": "simr random effects I",
    "text": "simr random effects I\n\n# this is on the variance/covariance scale\nVarCorr(mm2) <- diag(subj_sd^2)\nsummary(mm2)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 + x | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  3545.3   3574.8  -1766.7   3533.3      994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1792 -0.6596 -0.0057  0.6784  3.5150 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subj     (Intercept) 0.250    0.500        \n          x           0.090    0.300    0.00\n Residual             1.999    1.414        \nNumber of obs: 1000, groups:  subj, 26\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  1.000000   0.049771   20.09\nx           42.000000   0.008121 5171.53\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.117 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "02-simulation-in-R.html#simr-random-effects-ii",
    "href": "02-simulation-in-R.html#simr-random-effects-ii",
    "title": "Simulation in R",
    "section": "simr random effects II",
    "text": "simr random effects II\n\n# this is on the variance/covariance scale\nsdcovar <- diag(subj_sd)\nsdcovar[1,2] <- 0.1 # only need to fill in the upper triangle for simr\n# sdcor2cov is an experimental function in lme4\nVarCorr(mm2) <- sdcor2cov(sdcovar)\nsummary(mm2)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 + x | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  3545.3   3574.8  -1766.7   3533.3      994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1792 -0.6596 -0.0057  0.6784  3.5150 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subj     (Intercept) 0.250    0.500        \n          x           0.090    0.300    0.10\n Residual             1.999    1.414        \nNumber of obs: 1000, groups:  subj, 26\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept)  1.000000   0.049771   20.09\nx           42.000000   0.008121 5171.53\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.117 \noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "02-simulation-in-R.html#simr-also-provides-a-way-to-do-this-directly.",
    "href": "02-simulation-in-R.html#simr-also-provides-a-way-to-do-this-directly.",
    "title": "Simulation in R",
    "section": "simr also provides a way to do this directly….",
    "text": "simr also provides a way to do this directly….\n\nmm2alt <- makeLmer(form, beta, list(subject=sdcor2cov(sdcovar)), sig, dat)"
  },
  {
    "objectID": "02-simulation-in-R.html#did-it-work",
    "href": "02-simulation-in-R.html#did-it-work",
    "title": "Simulation in R",
    "section": "Did it work???",
    "text": "Did it work???\n\ndat$y <- simulate(mm2alt)[,1]\nsummary(lmer(form, dat, REML=FALSE))\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: y ~ 1 + x + (1 + x | subj)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  3747.4   3776.9  -1867.7   3735.4      994 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4317 -0.6577  0.0009  0.6404  3.1423 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n subj     (Intercept) 0.1669   0.4086       \n          x           0.1048   0.3237   0.45\n Residual             2.1342   1.4609       \nNumber of obs: 1000, groups:  subj, 26\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   1.0141     0.0927   10.94\nx            42.0402     0.0640  656.84\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.388"
  },
  {
    "objectID": "02-simulation-in-R.html#what-about-when-we-have-more-complex-data",
    "href": "02-simulation-in-R.html#what-about-when-we-have-more-complex-data",
    "title": "Simulation in R",
    "section": "What about when we have more complex data?",
    "text": "What about when we have more complex data?\n\nlisteners (subjects) listened to speakers make either a positive or a negative statement\neach statement (item) appeared either as a positive or a negative statement\nwe only had a small number of speakers, so modelling speaker-age or speaker-idiosyncraticities (by-speaker random effects) probably won’t be meaningful or useful\nhowever, we had more than one speaker of each gender, so speaker-gender and speaker-idiosyncraticity isn’t completely confounded\nwe had a fairly large number of speakers\nwe had a fairly large number of items\nhow do speaker attributes, listener attributes, and the type of statement interact to influence listener response times in some task?"
  },
  {
    "objectID": "02-simulation-in-R.html#create-the-ground-truth",
    "href": "02-simulation-in-R.html#create-the-ground-truth",
    "title": "Simulation in R",
    "section": "Create the ground truth",
    "text": "Create the ground truth\n\ngroundhog.library(\"tidyverse\", \"2022-07-14\")\n\nRegistered S3 methods overwritten by 'readr':\n  method                    from \n  as.data.frame.spec_tbl_df vroom\n  as_tibble.spec_tbl_df     vroom\n  format.col_spec           vroom\n  print.col_spec            vroom\n  print.collector           vroom\n  print.date_names          vroom\n  print.locale              vroom\n  str.col_spec              vroom\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n✔ purrr   0.3.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand()  masks Matrix::expand()\n✖ dplyr::filter()  masks stats::filter()\n✖ stringr::fixed() masks simr::fixed()\n✖ dplyr::lag()     masks stats::lag()\n✖ tidyr::pack()    masks Matrix::pack()\n✖ dplyr::recode()  masks car::recode()\n✖ purrr::some()    masks car::some()\n✖ tidyr::unpack()  masks Matrix::unpack()\n\n\n\u001b[36mSuccesfully attached 'tidyverse_1.3.1'\u001b[0m\n\nset.seed(42)\n\n# ==== Generate the Design ===\n# generate our sentences\nn_sentences <- 60\nsentences <- data.frame(sentence=sprintf(\"sent%02d\",1:n_sentences),\n                        # balanced: half positive, half negative\n                        positive_statement=sample(c(\"yes\", \"no\"), n_sentences,\n                                                  replace=TRUE, prob=c(0.5, 0.5)))\n\n# generate our speakers and determine which speakers spoke which sentences\nn_spkrs <- 4\n# all speakers produced all sentences\nspkrs_sentences <- expand.grid(spkr=sprintf(\"spkr%02d\",1:n_spkrs),\n                               sentence=sprintf(\"sent%02d\",1:n_sentences))\n\n# speakers only produced half of all sentences\nspkrs_sentences_1 <- expand.grid(spkr=sprintf(\"spkr%02d\",1:(n_spkrs/2)),\n                                 sentence=sprintf(\"sent%02d\",1:(n_sentences/2)))\nspkrs_sentences_2 <- expand.grid(spkr=sprintf(\"spkr%02d\",(n_spkrs/2 + 1):n_spkrs),\n                                 sentence=sprintf(\"sent%02d\",(n_sentences/2 +1):n_sentences))\nspkrs_sentences <- rbind(spkrs_sentences_1, spkrs_sentences_2)\n\nspkrs <- data.frame(spkr=sprintf(\"spkr%02d\",1:n_spkrs),\n                    spkr_age=round(runif(n_spkrs,min=18,max=50)),\n                    # keeping it simple for now\n                    spkr_gender=sample(c(\"female\", \"male\"), n_spkrs,\n                                                  replace=TRUE, prob=c(0.5, 0.5)))\nspkrs_sentences <- left_join(spkrs, spkrs_sentences, by=\"spkr\")\nstimuli <- left_join(spkrs_sentences, sentences, by=\"sentence\")\n\n# generate our subjects and match them to speakers\nn_subjs <- 35 # odd number! need integer division (%/%) later\nsubjs <- data.frame(subj=sprintf(\"subj%02d\",1:n_subjs),\n                    subj_age=round(runif(n_subjs,min=18,max=28)),\n                    # keeping it simple for now\n                    subj_gender=sample(c(\"female\", \"male\"), n_subjs,\n                                              replace=TRUE, prob=c(0.5, 0.5)))\nsubjs_sentences_1 <- expand.grid(spkr=sprintf(\"spkr%02d\",1:(n_spkrs/2)),\n                                 subj=sprintf(\"subj%02d\",1:(n_subjs %/% 2)))\nsubjs_sentences_2 <- expand.grid(spkr=sprintf(\"spkr%02d\",(n_spkrs/2 + 1):n_spkrs),\n                                 subj=sprintf(\"subj%02d\",(n_subjs %/% 2 +1):n_subjs))\nsubjs_sentences <- rbind(subjs_sentences_1, subjs_sentences_2)\n\nsubjs_sentences <- left_join(subjs, subjs_sentences, by=\"subj\")\n\nexperiment <- left_join(subjs_sentences, stimuli, by=\"spkr\")\n\n# === The Ground Truth Model ===\ntrue_form <- rt ~ 1 + positive_statement * spkr_gender * subj_age * subj_gender +\n             # listeners vary in their response to speaker attributes and statement polarity\n            (1 + positive_statement * spkr_age * spkr_gender| subj) +\n            # sentences have some idiosyncratic aspects that lead to different base RTs\n            # but don't otherwise differ based on speaker/listener features\n            (1 | sentence) # +\n            # speakers vary in the RT they elicit for a particular polarity, e.g.,\n            # by having different levels of associated affect/intonation\n            # but with so few speakers, we can't even begin to model this\n            # (1 + positive_statement | spkr)\n\n# set our contrasts, because nothing make sense without it!\nexperiment$positive_statement <- factor(experiment$positive_statement)\ncontrasts(experiment$positive_statement) <- contr.Sum(levels(experiment$positive_statement))\n\nexperiment$subj_gender <- factor(experiment$subj_gender)\ncontrasts(experiment$subj_gender) <- contr.Sum(levels(experiment$subj_gender))\n\nexperiment$spkr_gender <- factor(experiment$spkr_gender)\ncontrasts(experiment$spkr_gender) <- contr.Sum(levels(experiment$spkr_gender))\n\n\n# let's re-center our ages so that we're modelling the effect at age 25\n# instead of at age 0, when we look at the slopes and the intercepts\nexperiment$subj_age <- experiment$subj_age - 25\nexperiment$spkr_age <- experiment$spkr_age - 25\n\n# fixed effects, to get the coefficient names in the right order, use:\nmmat <- model.matrix(~ 1 + positive_statement * spkr_gender * subj_age * subj_gender, data=experiment)\nprint(colnames(mmat)) # 16 coefficients!\n\n [1] \"(Intercept)\"                                                                  \n [2] \"positive_statement[S.no]\"                                                     \n [3] \"spkr_gender[S.female]\"                                                        \n [4] \"subj_age\"                                                                     \n [5] \"subj_gender[S.female]\"                                                        \n [6] \"positive_statement[S.no]:spkr_gender[S.female]\"                               \n [7] \"positive_statement[S.no]:subj_age\"                                            \n [8] \"spkr_gender[S.female]:subj_age\"                                               \n [9] \"positive_statement[S.no]:subj_gender[S.female]\"                               \n[10] \"spkr_gender[S.female]:subj_gender[S.female]\"                                  \n[11] \"subj_age:subj_gender[S.female]\"                                               \n[12] \"positive_statement[S.no]:spkr_gender[S.female]:subj_age\"                      \n[13] \"positive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]\"         \n[14] \"positive_statement[S.no]:subj_age:subj_gender[S.female]\"                      \n[15] \"spkr_gender[S.female]:subj_age:subj_gender[S.female]\"                         \n[16] \"positive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]\"\n\n# everything in milliseconds\nbeta <- c(300, # (Intercept) 300ms base RT\n          +25, # positive_statement[S.no] 25ms slower RT than average for negative statements (=50ms slower than positive)\n          +10, # spkr_gender[S.female] 10ms slower RT than average for female speakers\n            5, # subj_age 5ms/year slower RT at age 26 (center+1) vs age 25 (center)\n            7, # subj_gender[S.female] 7ms slower RT than average for female listeners\n            3, # positive_statement[S.no]:spkr_gender[S.female]\n            1, # positive_statement[S.no]:subj_age\n            2, # spkr_gender[S.female]:subj_age\n           -4, # positive_statement[S.no]:subj_gender[S.female]\n            1, # spkr_gender[S.female]:subj_gender[S.female]\n           -1, # subj_age:subj_gender[S.female]\n            3, # positive_statement[S.no]:spkr_gender[S.female]:subj_age\n           -2, # positive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]\n           -3, # positive_statement[S.no]:subj_age:subj_gender[S.female]\n           -5, # spkr_gender[S.female]:subj_age:subj_gender[S.female]\n          0.5 # positive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]\n)\n# residual standard deviation is 25ms\n# this means that most trials are within ±50ms of the mean\nsig <- 25\n\n# we don't need to define n explicitly -- this emerged naturally from our design!\n\n# for now, we just let the RE correlations be zero because that makes our lives a lot\n# easier and I have no idea what plausible values for the correlations are\n\n# subject RE: (1 + positive_statement * spkr_age * spkr_gender| subj)\nsubj_mmat <- model.matrix(~ 1 + positive_statement * spkr_age * spkr_gender, experiment)\nprint(colnames(subj_mmat))\n\n[1] \"(Intercept)\"                                            \n[2] \"positive_statement[S.no]\"                               \n[3] \"spkr_age\"                                               \n[4] \"spkr_gender[S.female]\"                                  \n[5] \"positive_statement[S.no]:spkr_age\"                      \n[6] \"positive_statement[S.no]:spkr_gender[S.female]\"         \n[7] \"spkr_age:spkr_gender[S.female]\"                         \n[8] \"positive_statement[S.no]:spkr_age:spkr_gender[S.female]\"\n\nsubj_sd <- c(20, # (Intercept)\n             5, # positive_statement[S.no]\n             1, # spkr_age\n             2, # spkr_gender[S.female]\n             1, # positive_statement[S.no]:spkr_age\n             1, # positive_statement[S.no]:spkr_gender[S.female]\n             1, # spkr_age:spkr_gender[S.female]\n             1  # positive_statement[S.no]:spkr_age:spkr_gender[S.female])\n)\nsubj_sd <- diag(subj_sd)\n\n# sentence RE: (1|sentence)\nsentence_sd <- as.matrix(5)\n\n# speaker RE: (1 + positive_statement | spkr)\nspkr_sd <- c(5, # (Intercept)\n             3 # positive_statement[S.no]\n)\n\nspkr_sd <- diag(spkr_sd)\n\n# there's a bug in simr -- if you get an error or things don't align,\n# then try re-arranging the order so that things are ordered by decreasing number of levels:\n# sentences (60), subjects (35), speakers (4)\n# can't include speakers here because there are just too few to do anything meaningful\nmm_sim <- makeLmer(true_form, beta,\n                     list(sentence=sdcor2cov(sentence_sd),\n                          subj=sdcor2cov(subj_sd)),\n                          #spkr=sdcor2cov(spkr_sd)),\n                     sig,\n                     experiment)\n# check to make sure things match up with your settings above\nprint(VarCorr(mm_sim))\n\n Groups   Name                                                    Std.Dev.\n sentence (Intercept)                                              5      \n subj     (Intercept)                                             20      \n          positive_statement[S.no]                                 5      \n          spkr_age                                                 1      \n          spkr_gender[S.female]                                    2      \n          positive_statement[S.no]:spkr_age                        1      \n          positive_statement[S.no]:spkr_gender[S.female]           1      \n          spkr_age:spkr_gender[S.female]                           1      \n          positive_statement[S.no]:spkr_age:spkr_gender[S.female]  1      \n Residual                                                         25      \n Corr                                     \n                                          \n                                          \n 0.000                                    \n 0.000 0.000                              \n 0.000 0.000 0.000                        \n 0.000 0.000 0.000 0.000                  \n 0.000 0.000 0.000 0.000 0.000            \n 0.000 0.000 0.000 0.000 0.000 0.000      \n 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n                                          \n\nprint(fixef(mm_sim))\n\n                                                                  (Intercept) \n                                                                        300.0 \n                                                     positive_statement[S.no] \n                                                                         25.0 \n                                                        spkr_gender[S.female] \n                                                                         10.0 \n                                                                     subj_age \n                                                                          5.0 \n                                                        subj_gender[S.female] \n                                                                          7.0 \n                               positive_statement[S.no]:spkr_gender[S.female] \n                                                                          3.0 \n                                            positive_statement[S.no]:subj_age \n                                                                          1.0 \n                                               spkr_gender[S.female]:subj_age \n                                                                          2.0 \n                               positive_statement[S.no]:subj_gender[S.female] \n                                                                         -4.0 \n                                  spkr_gender[S.female]:subj_gender[S.female] \n                                                                          1.0 \n                                               subj_age:subj_gender[S.female] \n                                                                         -1.0 \n                      positive_statement[S.no]:spkr_gender[S.female]:subj_age \n                                                                          3.0 \n         positive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female] \n                                                                         -2.0 \n                      positive_statement[S.no]:subj_age:subj_gender[S.female] \n                                                                         -3.0 \n                         spkr_gender[S.female]:subj_age:subj_gender[S.female] \n                                                                         -5.0 \npositive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female] \n                                                                          0.5"
  },
  {
    "objectID": "02-simulation-in-R.html#simulate-some-data",
    "href": "02-simulation-in-R.html#simulate-some-data",
    "title": "Simulation in R",
    "section": "Simulate some data",
    "text": "Simulate some data\n\n# note if you change your design/setup, you can use the newdata argument to\n# simulate with the model you have  instead of needing to use makeLmermod again\nexperiment$rt <- simulate(mm_sim)[, 1]"
  },
  {
    "objectID": "02-simulation-in-R.html#fitting-the-true-model",
    "href": "02-simulation-in-R.html#fitting-the-true-model",
    "title": "Simulation in R",
    "section": "Fitting the True Model",
    "text": "Fitting the True Model\n\nmm_true <- lmer(true_form, data=experiment, REML=FALSE, control=lmerControl(calc.derivs=FALSE))\nsummary(mm_true)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: rt ~ 1 + positive_statement * spkr_gender * subj_age * subj_gender +  \n    (1 + positive_statement * spkr_age * spkr_gender | subj) +  \n    (1 | sentence)\n   Data: experiment\nControl: lmerControl(calc.derivs = FALSE)\n\n     AIC      BIC   logLik deviance df.resid \n 20213.2  20518.3 -10052.6  20105.2     2046 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.8824 -0.6560 -0.0067  0.6626  3.2480 \n\nRandom effects:\n Groups   Name                                                    Variance\n sentence (Intercept)                                               35.192\n subj     (Intercept)                                              762.052\n          positive_statement[S.no]                                 920.627\n          spkr_age                                                  10.757\n          spkr_gender[S.female]                                   1769.222\n          positive_statement[S.no]:spkr_age                          7.166\n          positive_statement[S.no]:spkr_gender[S.female]          1329.420\n          spkr_age:spkr_gender[S.female]                            13.326\n          positive_statement[S.no]:spkr_age:spkr_gender[S.female]    5.112\n Residual                                                          658.257\n Std.Dev. Corr                                     \n  5.932                                            \n 27.605                                            \n 30.342    0.03                                    \n  3.280   -0.93 -0.40                              \n 42.062    0.56  0.17 -0.55                        \n  2.677    0.33 -0.90  0.01 -0.08                  \n 36.461   -0.69 -0.24  0.71 -0.47 -0.01            \n  3.651   -0.68 -0.09  0.63 -0.98 -0.04  0.55      \n  2.261    0.69  0.06 -0.63  0.30  0.15 -0.91 -0.41\n 25.657                                            \nNumber of obs: 2100, groups:  sentence, 60; subj, 35\n\nFixed effects:\n                                                                               Estimate\n(Intercept)                                                                   301.31691\npositive_statement[S.no]                                                       31.71808\nspkr_gender[S.female]                                                          15.99974\nsubj_age                                                                        6.40203\nsubj_gender[S.female]                                                          15.17252\npositive_statement[S.no]:spkr_gender[S.female]                                 -6.62156\npositive_statement[S.no]:subj_age                                               1.96126\nspkr_gender[S.female]:subj_age                                                  3.79627\npositive_statement[S.no]:subj_gender[S.female]                                 -3.76240\nspkr_gender[S.female]:subj_gender[S.female]                                    -6.43096\nsubj_age:subj_gender[S.female]                                                 -0.09224\npositive_statement[S.no]:spkr_gender[S.female]:subj_age                         1.09733\npositive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]            0.49864\npositive_statement[S.no]:subj_age:subj_gender[S.female]                        -1.26829\nspkr_gender[S.female]:subj_age:subj_gender[S.female]                           -6.76054\npositive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]   1.64842\n                                                                              Std. Error\n(Intercept)                                                                      6.29220\npositive_statement[S.no]                                                         4.72930\nspkr_gender[S.female]                                                            2.91466\nsubj_age                                                                         1.72329\nsubj_gender[S.female]                                                            6.22749\npositive_statement[S.no]:spkr_gender[S.female]                                   4.27225\npositive_statement[S.no]:subj_age                                                1.28710\nspkr_gender[S.female]:subj_age                                                   0.81127\npositive_statement[S.no]:subj_gender[S.female]                                   4.63979\nspkr_gender[S.female]:subj_gender[S.female]                                      2.91386\nsubj_age:subj_gender[S.female]                                                   1.72301\npositive_statement[S.no]:spkr_gender[S.female]:subj_age                          1.17265\npositive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]             4.27097\npositive_statement[S.no]:subj_age:subj_gender[S.female]                          1.28676\nspkr_gender[S.female]:subj_age:subj_gender[S.female]                             0.81124\npositive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]    1.17264\n                                                                              t value\n(Intercept)                                                                    47.887\npositive_statement[S.no]                                                        6.707\nspkr_gender[S.female]                                                           5.489\nsubj_age                                                                        3.715\nsubj_gender[S.female]                                                           2.436\npositive_statement[S.no]:spkr_gender[S.female]                                 -1.550\npositive_statement[S.no]:subj_age                                               1.524\nspkr_gender[S.female]:subj_age                                                  4.679\npositive_statement[S.no]:subj_gender[S.female]                                 -0.811\nspkr_gender[S.female]:subj_gender[S.female]                                    -2.207\nsubj_age:subj_gender[S.female]                                                 -0.054\npositive_statement[S.no]:spkr_gender[S.female]:subj_age                         0.936\npositive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]            0.117\npositive_statement[S.no]:subj_age:subj_gender[S.female]                        -0.986\nspkr_gender[S.female]:subj_age:subj_gender[S.female]                           -8.334\npositive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]   1.406\n\n\n\nCorrelation matrix not shown by default, as p = 16 > 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it"
  },
  {
    "objectID": "02-simulation-in-R.html#fitting-a-reduced-model",
    "href": "02-simulation-in-R.html#fitting-a-reduced-model",
    "title": "Simulation in R",
    "section": "Fitting a Reduced Model",
    "text": "Fitting a Reduced Model\n\nform <- rt ~ 1 + positive_statement * spkr_gender * subj_age * subj_gender +\n            (1 + positive_statement + spkr_age + spkr_gender| subj) +\n            (1 | sentence)\nmm_reduced <- lmer(form, data=experiment, REML=FALSE, control=lmerControl(calc.derivs=FALSE))\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(mm_reduced)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: rt ~ 1 + positive_statement * spkr_gender * subj_age * subj_gender +  \n    (1 + positive_statement + spkr_age + spkr_gender | subj) +  \n    (1 | sentence)\n   Data: experiment\nControl: lmerControl(calc.derivs = FALSE)\n\n     AIC      BIC   logLik deviance df.resid \n 20774.6  20932.8 -10359.3  20718.6     2072 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1744 -0.6412 -0.0053  0.6160  3.4743 \n\nRandom effects:\n Groups   Name                     Variance Std.Dev. Corr             \n sentence (Intercept)                27.23   5.219                    \n subj     (Intercept)              5177.37  71.954                    \n          positive_statement[S.no]  441.08  21.002   -0.42            \n          spkr_age                   20.64   4.543   -0.94  0.34      \n          spkr_gender[S.female]     954.42  30.894   -0.97  0.20  0.88\n Residual                           939.65  30.654                    \nNumber of obs: 2100, groups:  sentence, 60; subj, 35\n\nFixed effects:\n                                                                                Estimate\n(Intercept)                                                                   298.224040\npositive_statement[S.no]                                                       32.610541\nspkr_gender[S.female]                                                          20.174120\nsubj_age                                                                        6.349511\nsubj_gender[S.female]                                                          14.759971\npositive_statement[S.no]:spkr_gender[S.female]                                 -5.598837\npositive_statement[S.no]:subj_age                                               1.499034\nspkr_gender[S.female]:subj_age                                                  3.848352\npositive_statement[S.no]:subj_gender[S.female]                                 -4.607826\nspkr_gender[S.female]:subj_gender[S.female]                                    -5.293966\nsubj_age:subj_gender[S.female]                                                 -0.007882\npositive_statement[S.no]:spkr_gender[S.female]:subj_age                         1.440845\npositive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]            1.111756\npositive_statement[S.no]:subj_age:subj_gender[S.female]                        -1.101744\nspkr_gender[S.female]:subj_age:subj_gender[S.female]                           -6.909360\npositive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]   1.595064\n                                                                              Std. Error\n(Intercept)                                                                     6.767985\npositive_statement[S.no]                                                        5.759924\nspkr_gender[S.female]                                                           5.092692\nsubj_age                                                                        1.844771\nsubj_gender[S.female]                                                           6.719817\npositive_statement[S.no]:spkr_gender[S.female]                                  1.082132\npositive_statement[S.no]:subj_age                                               1.562707\nspkr_gender[S.female]:subj_age                                                  1.384717\npositive_statement[S.no]:subj_gender[S.female]                                  5.717706\nspkr_gender[S.female]:subj_gender[S.female]                                     5.092508\nsubj_age:subj_gender[S.female]                                                  1.844587\npositive_statement[S.no]:spkr_gender[S.female]:subj_age                         0.295980\npositive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]            1.082131\npositive_statement[S.no]:subj_age:subj_gender[S.female]                         1.562206\nspkr_gender[S.female]:subj_age:subj_gender[S.female]                            1.384716\npositive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]   0.295980\n                                                                              t value\n(Intercept)                                                                    44.064\npositive_statement[S.no]                                                        5.662\nspkr_gender[S.female]                                                           3.961\nsubj_age                                                                        3.442\nsubj_gender[S.female]                                                           2.196\npositive_statement[S.no]:spkr_gender[S.female]                                 -5.174\npositive_statement[S.no]:subj_age                                               0.959\nspkr_gender[S.female]:subj_age                                                  2.779\npositive_statement[S.no]:subj_gender[S.female]                                 -0.806\nspkr_gender[S.female]:subj_gender[S.female]                                    -1.040\nsubj_age:subj_gender[S.female]                                                 -0.004\npositive_statement[S.no]:spkr_gender[S.female]:subj_age                         4.868\npositive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]            1.027\npositive_statement[S.no]:subj_age:subj_gender[S.female]                        -0.705\nspkr_gender[S.female]:subj_age:subj_gender[S.female]                           -4.990\npositive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]   5.389\n\n\n\nCorrelation matrix not shown by default, as p = 16 > 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')"
  },
  {
    "objectID": "02-simulation-in-R.html#even-the-reduced-model-is-too-complex-for-the-amount-of-information-we-have",
    "href": "02-simulation-in-R.html#even-the-reduced-model-is-too-complex-for-the-amount-of-information-we-have",
    "title": "Simulation in R",
    "section": "Even the reduced model is too complex for the amount of information we have!",
    "text": "Even the reduced model is too complex for the amount of information we have!\n\nsummary(rePCA(mm_reduced))\n\n$sentence\nImportance of components:\n                         [,1]\nStandard deviation     0.1702\nProportion of Variance 1.0000\nCumulative Proportion  1.0000\n\n$subj\nImportance of components:\n                        [,1]    [,2]    [,3] [,4]\nStandard deviation     2.561 0.66250 0.13218    0\nProportion of Variance 0.935 0.06255 0.00249    0\nCumulative Proportion  0.935 0.99751 1.00000    1"
  },
  {
    "objectID": "02-simulation-in-R.html#cant-do-it-just-once-.",
    "href": "02-simulation-in-R.html#cant-do-it-just-once-.",
    "title": "Simulation in R",
    "section": "Can’t do it just once ….",
    "text": "Can’t do it just once ….\n\nBut doing things 1000x for each of a dozen different settings is a LOT of computation\nR has lots of nice features and it’s gotten faster, but…\nJulia is faster"
  },
  {
    "objectID": "03-simulation-and-the-bootstrap-in-Julia.html",
    "href": "03-simulation-and-the-bootstrap-in-Julia.html",
    "title": "Simulation in Julia",
    "section": "",
    "text": "using AlgebraOfGraphics\nusing CairoMakie\nusing DataFrames\nusing GLM\nusing MixedModels\nusing MixedModelsMakie\nusing MixedModelsExtras\nusing MixedModelsSim\nusing ProgressMeter\nusing StatsModels\nusing StatsBase\nusing Random\n\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);\n\n\nkb07 = MixedModels.dataset(:kb07)\ncontrasts = Dict(:subj => Grouping(),\n                 :item => Grouping(),\n                 :spkr => EffectsCoding(),\n                 :prec => EffectsCoding(),\n                 :load => EffectsCoding())\nform = @formula(rt_trunc ~ 1 + spkr * prec * load +\n                          (1 + spkr * prec * load | subj) +\n                          (1 + spkr * prec * load | item))\nmodel = fit(MixedModel, form, kb07; contrasts)\n\nMinimizing 1697      Time: 0:00:02 ( 1.63 ms/it)\n  objective:  28578.379882306126\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2181.8600\n76.9837\n28.34\n<1e-99\n301.6141\n361.2191\n\n\nspkr: old\n67.7485\n19.2522\n3.52\n0.0004\n71.7552\n41.6368\n\n\nprec: maintain\n-333.9212\n47.6946\n-7.00\n<1e-11\n75.0742\n249.7744\n\n\nload: yes\n78.5831\n21.2565\n3.70\n0.0002\n87.5743\n53.7719\n\n\nspkr: old & prec: maintain\n-21.7784\n20.4371\n-1.07\n0.2866\n95.0200\n31.9584\n\n\nspkr: old & load: yes\n18.3844\n17.3845\n1.06\n0.2903\n42.6824\n38.0149\n\n\nprec: maintain & load: yes\n4.5339\n22.4912\n0.20\n0.8402\n86.6206\n68.6652\n\n\nspkr: old & prec: maintain & load: yes\n23.4202\n21.2220\n1.10\n0.2698\n62.3229\n70.7639\n\n\nResidual\n633.7265\n\n\n\n\n\n\n\n\n\n\n\nmodel.rePCA\n\n(subj = [0.37031238827926216, 0.6417807855784552, 0.8792429052743644, 0.9999742210400684, 0.9999997919569437, 0.9999999996158602, 1.0, 1.0], item = [0.3557735142771573, 0.6333318296284408, 0.8146986374406038, 0.9481639067412695, 0.9998220284927168, 0.9999998992983445, 1.0, 1.0])\n\n\n\nshrinkageplot(model, :subj)\n\n\n\n\n\nshrinkageplot(model, :item)\n\n\n\n\n\nrng = MersenneTwister(42)\ndat = DataFrame(kb07; copycols=true)\n\nsimple_form = @formula(rt_trunc ~ 1 + spkr * prec * load +\n                          (1 + spkr + prec + load | subj) +\n                          (1 + spkr + prec + load | item))\n\nresults = DataFrame()\n\nsimple_model = fit(MixedModel, simple_form, kb07; contrasts)\n\n# if doing this yourself, add\n# @showprogress\n# before the for-loop and get an automatic progress bar\n# courtesy of ProgressMeter.jl\nfor i in 1:100\n    refit!(simple_model, simulate(rng, model); progress=false)\n    est = DataFrame(coeftable(simple_model))\n    est[!, :iter] .= i\n    append!(results, est)\nend\n\nrename!(results,\n        \"Name\" => \"coef\",\n        \"Coef.\" => \"est\",\n        \"Std. Error\" => \"se\",\n         \"Pr(>|z|)\" => \"p\")\n\nMinimizing 723   Time: 0:00:00 ( 0.48 ms/it)\n  objective:  28637.1393507629\n\n\n\n800 rows × 6 columns (omitted printing of 1 columns)coefestsezpStringFloat64Float64Float64Float641(Intercept)2174.2670.853230.68688.54547e-2072spkr: old68.051920.9113.254370.001136453prec: maintain-334.19345.3748-7.365181.76904e-134load: yes58.454720.00042.922670.003470445spkr: old & prec: maintain-5.9908215.512-0.3862050.6993456spkr: old & load: yes21.03715.5121.356180.1750437prec: maintain & load: yes12.303715.5120.793170.4276798spkr: old & prec: maintain & load: yes14.81815.5120.955260.3394469(Intercept)2179.6175.868228.7291.65906e-18110spkr: old80.343720.07744.00176.28895e-511prec: maintain-296.96748.7262-6.09461.09708e-912load: yes128.86518.84316.838847.98346e-1213spkr: old & prec: maintain4.5711316.08160.2842460.77622214spkr: old & load: yes8.514416.08160.529450.59649315prec: maintain & load: yes-35.197616.0816-2.188690.028619316spkr: old & prec: maintain & load: yes7.4980916.08160.4662530.64103417(Intercept)2189.8772.101830.37191.29155e-20218spkr: old96.435722.09574.364451.27445e-519prec: maintain-353.00150.2821-7.020422.2121e-1220load: yes65.137823.11012.818580.0048236621spkr: old & prec: maintain3.7820615.79910.2393840.81080822spkr: old & load: yes-5.2055115.7991-0.3294820.74179223prec: maintain & load: yes25.038415.79911.58480.11301224spkr: old & prec: maintain & load: yes37.795715.79912.392270.016744525(Intercept)2186.4681.125526.95155.47511e-16026spkr: old86.85821.79733.984816.75331e-527prec: maintain-329.51649.0542-6.717381.85021e-1128load: yes88.554820.73634.270511.95025e-529spkr: old & prec: maintain-61.277416.0246-3.823960.00013132430spkr: old & load: yes17.835616.02461.113020.265702⋮⋮⋮⋮⋮⋮\n\n\n\nplt = data(results) * mapping(:est; layout=:coef) * AlgebraOfGraphics.density()\ndraw(plt)\n\n\n\n\n\ncombine(groupby(results, :coef), :est => shortestcovint => :est)\n\n\n8 rows × 2 columnscoefestStringTuple…1(Intercept)(2027.76, 2317.46)2spkr: old(25.8411, 97.5038)3prec: maintain(-410.845, -236.202)4load: yes(29.3886, 114.582)5spkr: old & prec: maintain(-66.9195, 10.8659)6spkr: old & load: yes(-8.39011, 58.4824)7prec: maintain & load: yes(-30.6927, 50.2594)8spkr: old & prec: maintain & load: yes(-36.9853, 52.137)\n\n\n\ncombine(groupby(results, :coef),\n        :p => (x -> mean(x .< 0.05)) => :percent_significant)\n\n\n8 rows × 2 columnscoefpercent_significantStringFloat641(Intercept)1.02spkr: old0.913prec: maintain1.04load: yes0.945spkr: old & prec: maintain0.346spkr: old & load: yes0.37prec: maintain & load: yes0.218spkr: old & prec: maintain & load: yes0.36"
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Introduction to Simulation",
    "section": "",
    "text": "2020–now Beacon Biosignals\n2017–2020 MPI for Psycholinguistics\n2015–2017 University of South Australia\n2015 Dr.Phil. Linguistik, Philipps-Universität Marburg\n2010 MA Germanistische Linguistik, Philipps-Universität Marburg\n2008 BA German Language and Literature, University of Notre Dame (USA)\n2008 BS Mathematics, University of Notre Dame (USA)\n\nalso: minor contributor to lme4, occasionally prolific contributor to R-SIG-mixed-models, major contributor to MixedModels.jl\n\n\n\nThis is waaaay more than we can do in a single day, but we’ll spend as much time as needed on the fundamentals and pick and choose the advanced topics based on that.\n\n\n\nA fair amount of math, sorry-not-sorry\nModel notation and what that means\n\nwhere are the “random” bits?\nwhere are the assumptions?\n\nHow mixed models differ from classical regression and what that means for things like degrees of freedom and hence p-values\nReminder about contrast coding\n\n\n\n\n\n\n\n\nThe parametric bootstrap is a simulation!\nSimulation-based power analysis can use the parametric bootstrap\nJulia is great …..\n\n\n\n\n\nCreating “imperfect” simulated data to understand the impact of the real-world on your ideal mdoel\nEmbracing uncertainty\n\n\n\n\n\nWhat does it mean for the model to be rank deficient/singular/a boundary fit?\nPractical impacts of rank deficiency for various tests/convergence warnings \nVariance-bias tradeoff; over- vs. underfitting and what that means for your inference.\n\n\n\n\n\nLower power\nLink functions and a different observation-level random component\nConditional vs. marginal effects"
  },
  {
    "objectID": "01-introduction.html#the-general-linear-model",
    "href": "01-introduction.html#the-general-linear-model",
    "title": "Introduction to Simulation",
    "section": "The general linear model",
    "text": "The general linear model\n\\[ y_i = \\beta_{1}x_{i,1} + \\beta_{0} + \\varepsilon_i \\]\n\nIndependent variable(s) measured without error\nDependent variable a linear function of independent variable plus some error / random variation\nAssumptions about the distribution of the error very important!"
  },
  {
    "objectID": "01-introduction.html#some-of-the-assumptions-of-linear-regression",
    "href": "01-introduction.html#some-of-the-assumptions-of-linear-regression",
    "title": "Introduction to Simulation",
    "section": "Some of the assumptions of linear regression",
    "text": "Some of the assumptions of linear regression\n\nindependence of errors\nequal variance of errors (homoskedacity)\nnormality of errors\n\n\nAnd that’s roughly the order of importance!"
  },
  {
    "objectID": "01-introduction.html#observation-level",
    "href": "01-introduction.html#observation-level",
    "title": "Introduction to Simulation",
    "section": "Observation level",
    "text": "Observation level\n\n“Error terms” formulation\n\\[ y_i = \\beta_{0} + \\beta_{1}x_{i,1} + \\ldots + \\beta_{p}x_{i,p} + \\varepsilon_i \\] \\[ \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\]\nfor the \\(i\\)th observation (of \\(n\\) total) of \\(p\\) predictors\n\n\n“Distribution” formulation\n\\[ y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\] \\[ \\mu_i =\n\\beta_{0} + \\beta_{1}x_{i,1} + \\ldots + \\beta_{p}x_{i,p} \\]\nThis was inspired by a great blog post by Rasmus Bååth, which you really should read."
  },
  {
    "objectID": "01-introduction.html#long-sums-over-lots-of-observations-are-annoying",
    "href": "01-introduction.html#long-sums-over-lots-of-observations-are-annoying",
    "title": "Introduction to Simulation",
    "section": "Long sums over lots of observations are annoying",
    "text": "Long sums over lots of observations are annoying\n\nWriting \\(\\beta_{0} + \\beta_{1}x_{i,1} + \\ldots + \\beta_{p}x_{i,p}\\) gets old fast.\nso often use sigma notation: \\[ \\beta_0 + \\sum_{j=1}^{p}  \\beta_{j}x_{i,j} = \\beta_{0} + \\beta_{1}x_{i,1} + \\ldots + \\beta_{p}x_{i,p}\\]\nbut even this is annoying"
  },
  {
    "objectID": "01-introduction.html#matrices-are-convenient",
    "href": "01-introduction.html#matrices-are-convenient",
    "title": "Introduction to Simulation",
    "section": "Matrices are convenient",
    "text": "Matrices are convenient\nway to express a system of equations resulting from \\(n\\) observations with \\(p\\) predictors/covariates:\n\\[\\begin{equation*}\n%\\underbrace{%\n\\overbrace{%\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_{n-1} \\\\\ny_n \\\\\n\\end{bmatrix}}^{\\textstyle n \\times 1}\n=\n\\overbrace{%\n\\begin{bmatrix}\n1 & x_{1,1} & \\ldots & x_{1,p} \\\\\n1 & x_{2,1} & \\ldots & x_{2,p} \\\\\n1 & x_{3,1} & \\ldots & x_{3,p} \\\\\n& & \\vdots & \\\\\n1 & x_{n-1,1} & \\ldots & x_{n-1,p} \\\\\n1 & x_{n,1} & \\ldots & x_{n,p} \\\\\n\\end{bmatrix}}^{\\textstyle n \\times p}\n\\overbrace{%\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p \\\\\n\\end{bmatrix}}^{\\textstyle p \\times 1}\n+\n\\overbrace{%\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\varepsilon_3 \\\\\n\\vdots \\\\\n\\varepsilon_{n-1} \\\\\n\\varepsilon_n \\\\\n\\end{bmatrix}}^{\\textstyle n \\times 1}%\n%}_{\\textstyle\\bf Y = X\\beta + \\varepsilon}\n\\end{equation*}\\]\nbecomes \\(y = X\\beta + \\varepsilon\\)"
  },
  {
    "objectID": "01-introduction.html#matrix-level",
    "href": "01-introduction.html#matrix-level",
    "title": "Introduction to Simulation",
    "section": "Matrix level",
    "text": "Matrix level\n\n“Error terms” formulation\n\\[Y = X\\beta + \\varepsilon\\] \\[\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\]\n\n\n“Distribution” formulation\n\\[Y \\sim \\mathcal{N}(\\mu,\\sigma^2)\\] \\[\\mu = X\\beta \\]\n\nWe usually learn and conceive of the linear model in terms of “line of best fit” plus some “error” so the distribution formulation seems a bit weird at first.\n\nIt is much more convenient when we start looking at extensions of linear regression, such as the linear mixed-effects model:\n\\[\\begin{align*}\n(Y | B = b ) &\\sim \\mathcal{N}( X\\beta + Zb, \\sigma^2 I ) \\\\\n  B &\\sim \\mathcal{N}(0, \\Sigma _\\theta)\n\\end{align*}\\]"
  },
  {
    "objectID": "01-introduction.html#what-does-it-mean-to-fit-a-classical-linear-model",
    "href": "01-introduction.html#what-does-it-mean-to-fit-a-classical-linear-model",
    "title": "Introduction to Simulation",
    "section": "What does it mean to fit a (classical linear) model?",
    "text": "What does it mean to fit a (classical linear) model?\n\\[Y \\sim \\mathcal{N}(\\mu,\\sigma^2)\\] \\[\\mu = X\\beta \\]\n\nMinimize sum of squares? (and what sum of squares are we talking about?)\nMaximize likelihood?\nMinimize deviance?\n\n\n\nAll equivalent for classical linear models\nAlso equivalent to finding \\(\\beta\\) that leads to the smallest \\(\\sigma^2\\)\nOrdinary least squares is the maximum likelihood estimator and BLUE (best, linear unbiased estimator )"
  },
  {
    "objectID": "01-introduction.html#what-is-likelihood-i",
    "href": "01-introduction.html#what-is-likelihood-i",
    "title": "Introduction to Simulation",
    "section": "What is likelihood? I",
    "text": "What is likelihood? I\nOne of the hardest parts about statistics (and math in general) is that everyday words are given precise technical meanings.\n\n\\(P(H|D)\\): posterior probability\n\\(P(D|H)\\): likelihood\n\nRelated via Bayes’ Theorem: \\[ P(H|D) = \\frac{P(D|H)P(H)}{P(D)}\\]"
  },
  {
    "objectID": "01-introduction.html#what-is-likelihood-ii",
    "href": "01-introduction.html#what-is-likelihood-ii",
    "title": "Introduction to Simulation",
    "section": "What is likelihood? II",
    "text": "What is likelihood? II\n\n… remember that “likelihood” is a technical term. The likelihood of \\(H\\), \\(Pr(O\\|H)\\), and the posterior probability of \\(H\\), \\(Pr(H\\|O)\\), are different quantities and they can have different values. The likelihood of \\(H\\) is the probability that \\(H\\) confers on \\(O\\), not the probability that \\(O\\) confers on \\(H\\). Suppose you hear a noise coming from the attic of your house. You consider the hypothesis that there are gremlins up there bowling. The likelihood of this hypothesis is very high, since if there are gremlins bowling in the attic, there probably will be noise. But surely you don’t think that the noise makes it very probable that there are gremlins up there bowling. … The gremlin hypothesis has a high likelihood (in the technical sense) but a low probability.\n\nSober, E. (2008). Evidence and Evolution: the Logic Behind the Science. Cambridge University Press."
  },
  {
    "objectID": "01-introduction.html#maximum-likelihood-estimation-mle",
    "href": "01-introduction.html#maximum-likelihood-estimation-mle",
    "title": "Introduction to Simulation",
    "section": "Maximum likelihood estimation (MLE)",
    "text": "Maximum likelihood estimation (MLE)\n\nfind the coefficients / predictor that maximizes the likelihood\ni.e. find the quantitative hypothesis which has the best chance of generating the observed data\nmany numerical, iterative techniques for doing this (implemented in R with glm())\nsome classes of models have a closed-form / easily / directly computable solution (e.g. ordinary least-squares, implemented in R with lm())\n\nMost of the methods you know are based on this technique."
  },
  {
    "objectID": "01-introduction.html#what-do-these-parts-all-mean-i",
    "href": "01-introduction.html#what-do-these-parts-all-mean-i",
    "title": "Introduction to Simulation",
    "section": "What do these parts all mean? I",
    "text": "What do these parts all mean? I\n\\[\\begin{align*}\n(Y | B = b ) &\\sim \\mathcal{N}(\\mu, \\sigma^2 I ) \\\\\n  \\mu &= X\\beta + Zb \\\\\n  B &\\sim \\mathcal{N}(0, \\Sigma _\\theta)\n\\end{align*}\\]\n\nThe reponse, conditional on the predicted mean \\(\\mu\\) is normally distributed\n\\(\\sigma^2 I\\) is homoskedacity in a multivariate context:\n\nsingle shared residual variance (\\(\\sigma^2\\))\nno autocorrelation or the like (everything happens on the diagonal \\(\\bf{I}\\))\n\n\\(Z\\) is a matrix of indicator variables for levels (individual membership) of the grouping variable\nThe predicted mean for an individual is thus the sum of the population-level effect (\\(X\\beta\\)) and the predicted offsets for that individual (\\(b\\))."
  },
  {
    "objectID": "01-introduction.html#what-do-these-parts-all-mean-ii",
    "href": "01-introduction.html#what-do-these-parts-all-mean-ii",
    "title": "Introduction to Simulation",
    "section": "What do these parts all mean? II",
    "text": "What do these parts all mean? II\n\\[\\begin{align*}\n(Y | B = b ) &\\sim \\mathcal{N}(\\mu, \\sigma^2 I ) \\\\\n  \\mu &= X\\beta + Zb \\\\\n  B &\\sim \\mathcal{N}(0, \\Sigma _\\theta)\n\\end{align*}\\]\n\nThe observed random effects \\(b\\) are draws from some latent distribution \\(B\\), which is normally distributed with mean 0 and some covariance structure \\(\\Sigma _\\theta\\).\nTechnically, we’re not estimating the \\(b\\), but rather predicting them. Instead, we’re estimating their covariance \\(\\Sigma _\\theta\\).\nThis is actually really profound and important because it means we don’t add additional parameters to the model when adding e.g. additional participants, items, etc.\nMore on this and its relationship to “shrinkage” in just a bit."
  },
  {
    "objectID": "01-introduction.html#how-does-math-match-a-model-in-rjulia-i",
    "href": "01-introduction.html#how-does-math-match-a-model-in-rjulia-i",
    "title": "Introduction to Simulation",
    "section": "How does math match a model in R/Julia? I",
    "text": "How does math match a model in R/Julia? I\n\\[\\begin{align*}\n(Y | B = b ) &\\sim \\mathcal{N}(\\mu, \\sigma^2 I ) \\\\\n  \\mu &= X\\beta + Zb \\\\\n  B &\\sim \\mathcal{N}(0, \\Sigma _\\theta)\n\\end{align*}\\]\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18"
  },
  {
    "objectID": "01-introduction.html#how-does-math-match-a-model-in-rjulia-ii",
    "href": "01-introduction.html#how-does-math-match-a-model-in-rjulia-ii",
    "title": "Introduction to Simulation",
    "section": "How does math match a model in R/Julia? II",
    "text": "How does math match a model in R/Julia? II\n\\[\\begin{align*}\n(Y | B = b ) &\\sim \\mathcal{N}(\\mu, \\sigma^2 I ) \\\\\n  \\mu &= X\\beta + Zb \\\\\n  B &\\sim \\mathcal{N}(0, \\Sigma _\\theta)\n\\end{align*}\\]\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(>|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    <1e-99\ndays          10.4673     1.50224   6.97    <1e-11\n──────────────────────────────────────────────────"
  },
  {
    "objectID": "01-introduction.html#what-does-the-term-random-effect-refer-to-anyway",
    "href": "01-introduction.html#what-does-the-term-random-effect-refer-to-anyway",
    "title": "Introduction to Simulation",
    "section": "What does the term “random effect” refer to anyway?",
    "text": "What does the term “random effect” refer to anyway?\nDepending on context, it can refer to:\n\nthe entire random variable \\(B\\) and associated design matrix \\(Z\\)\nthe estimated variance components and associated covariances/correlations \\(\\Sigma _\\theta\\)\nthe blocking/grouping variables (the stuff behind |)\nthe random slopes/intercepts (experimental variables which determine the structure of \\(Z\\)) associated with blocking variables (the stuff in front of |)\nthe associated predictions for the varying slopes/intercepts at each level of the blocking variables (the \\(b\\), a.k.a the BLUPs or conditional modes)\n\nThere are good reasons why all of these things get the same name in different contexts, so you just have to live with the ambiguity. But understanding that these are different will help you to be clear in your own writing and to understand where others may be having misunderstandings."
  },
  {
    "objectID": "01-introduction.html#why-variances-what-is-shrinkage",
    "href": "01-introduction.html#why-variances-what-is-shrinkage",
    "title": "Introduction to Simulation",
    "section": "Why variances? What is shrinkage?",
    "text": "Why variances? What is shrinkage?\n\\[\\begin{align*}\n(Y | B = b ) &\\sim \\mathcal{N}(\\mu, \\sigma^2 I ) \\\\\n  \\mu &= X\\beta + Zb \\\\\n  B &\\sim \\mathcal{N}(0, \\Sigma _\\theta)\n\\end{align*}\\]\n\nfewer parameters\neverything in one model step\nsharing of information between different levels of the grouping variables\n\n“sharing strength” (Tukey)\n“partial pooling”\nregularization / shrinkage towards the grand mean\n\nshared strength means better estimates for the weak\nin OLS, our goal was in some sense to minimize \\(\\sigma^2\\); here are goal in some sense is to jointly minimize \\(\\sigma^2\\) and \\(\\Sigma _\\theta\\)"
  },
  {
    "objectID": "01-introduction.html#regularization",
    "href": "01-introduction.html#regularization",
    "title": "Introduction to Simulation",
    "section": "Regularization",
    "text": "Regularization"
  },
  {
    "objectID": "01-introduction.html#shrinkage",
    "href": "01-introduction.html#shrinkage",
    "title": "Introduction to Simulation",
    "section": "Shrinkage",
    "text": "Shrinkage"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using Simulation to Understand Mixed Models in the Wild",
    "section": "",
    "text": "Real-world data present a number of challenges for data analysis. Simulation provides a way to examine the impact of the quirks of real-world data on your analysis. In this tutorial, I will introduce simulation with mixed models as a tool for planning your analysis (e.g., power analysis) and as a way to re-consider your plan after contact with real-world data (e.g., what impact does imbalance in my sample have on my inferences?). Simulation and real-world data analysis should not be rivals, but rather partners in inference."
  }
]