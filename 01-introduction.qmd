---
title: Introduction to Simulation
subtitle: Review of Mixed Models and Introduction to Model Notation
author: Phillip Alday
date: 6 May 2022
theme: Singapore
slide-level: 2
csl: apa.csl
lang: en-US
output:
    beamer_presentation:
        fig_width: 4.5
        fig_height: 3
        fig_caption: false
        pandoc_args: ["--wrap=none"]
---

# Preliminaries

## About Me

- 2020--now Beacon Biosignals
- 2017--2020 MPI for Psycholinguistics
- 2015--2017 University of South Australia
- 2015 Dr.Phil. Linguistik, Philipps-Universität Marburg
- 2010 MA Germanistische Linguistik, Philipps-Universität Marburg
- 2008 BA German Language and Literature, University of Notre Dame (USA)
- 2008 BS Mathematics, University of Notre Dame (USA)

also: minor contributor to lme4, occasionally prolific contributor to R-SIG-mixed-models,
major contributor to MixedModels.jl

## Plan {.shrink}

This is waaaay more than we can do in a single day, but we'll spend as much time as needed on the fundamentals and pick and choose the advanced topics based on that.

### What is a mixed model and how can we use that to design a simulation?
- A fair amount of math, sorry-not-sorry
- Model notation and what that means
    - where are the "random" bits?
    - where are the assumptions?
- How mixed models differ from classical regression and what that means for things like degrees of freedom and hence p-values
- Reminder about contrast coding

### Parametric bootstrap and power analysis
- The parametric bootstrap is a simulation!
- Simulation-based power analysis can use the parametric bootstrap
- Julia is great .....

### Imbalance, collinearity and rank deficiency in the fixed effects
- Creating "imperfect" simulated data to understand the impact of the real-world on your ideal mdoel
- Embracing uncertainty

### Rank deficiency in the random effects and random effects selection
- What does it mean for the model to be rank deficient/singular/a boundary fit?
- Practical impacts of rank deficiency for various tests/convergence
warnings  (see also lme4 v1.1-29 (2022-04-07))
- Variance-bias tradeoff; over- vs. underfitting and what that means for
your inference.

### Extending this all to GLMM
- Lower power
- Link functions and a different observation-level random component

# Back to basics


##
# How do you write your model?

## The general linear model

$$ y_i = \beta_{1}x_{i,1} + \beta_{0} + \varepsilon_i $$

* Independent variable(s) measured without error
* Dependent variable a linear function of independent variable plus some error / random variation
* Assumptions about the distribution of the error very important!

## Some of the assumptions of linear regression
* independence of *errors*
* equal variance of *errors* (homoskedacity)
* normality of *errors*

. . .

And that's roughly the order of importance!

## Observation level

### "Error terms" formulation
$$ y_i = \beta_{0} + \beta_{1}x_{i,1} + \ldots + \beta_{p}x_{i,p} + \varepsilon_i $$
$$ \varepsilon_i \sim \mathcal{N}(0, \sigma^2) $$

for the $i$th observation (of $n$ total) of $p$ predictors


. . .

### "Distribution" formulation

$$ y_i \sim \mathcal{N}(\mu_i, \sigma^2) $$
$$ \mu_i =
\beta_{0} + \beta_{1}x_{i,1} + \ldots + \beta_{p}x_{i,p} $$

\vfill \tiny
This was inspired by [a great blog post by Rasmus Bååth](http://www.sumsar.net/blog/2013/10/how-do-you-write-your-model-definitions/), which you really should read.


## Long sums over lots of observations are annoying
- Writing $\beta_{0} + \beta_{1}x_{i,1} + \ldots + \beta_{p}x_{i,p}$ gets old fast.
- so often use sigma notation:
    $$ \beta_0 + \sum_{j=1}^{p}  \beta_{j}x_{i,j} = \beta_{0} + \beta_{1}x_{i,1} + \ldots + \beta_{p}x_{i,p}$$
- but even this is annoying

## Matrices are convenient

\ldots way to express a system of equations resulting from $n$ observations with $p$ predictors/covariates:

\begin{equation*}
%\underbrace{%
\overbrace{%
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
\vdots \\
y_{n-1} \\
y_n \\
\end{bmatrix}}^{\textstyle n \times 1}
=
\overbrace{%
\begin{bmatrix}
1 & x_{1,1} & \ldots & x_{1,p} \\
1 & x_{2,1} & \ldots & x_{2,p} \\
1 & x_{3,1} & \ldots & x_{3,p} \\
& & \vdots & \\
1 & x_{n-1,1} & \ldots & x_{n-1,p} \\
1 & x_{n,1} & \ldots & x_{n,p} \\
\end{bmatrix}}^{\textstyle n \times p}
\overbrace{%
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p \\
\end{bmatrix}}^{\textstyle p \times 1}
+
\overbrace{%
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\varepsilon_3 \\
\vdots \\
\varepsilon_{n-1} \\
\varepsilon_n \\
\end{bmatrix}}^{\textstyle n \times 1}%
%}_{\textstyle\bf Y = X\beta + \varepsilon}
\end{equation*}

becomes $\bf y = X\beta + \varepsilon$

\note{
\begin{itemize}
\item note the ordering of $\beta$ and $X$ has changed here
\item matrix math isn't commutative
\item this is weird at first, but actually highlights that we're solving for the $\beta$s and `know' the $X$s, which is the opposite of school
\end{itemize}
}

## Matrix level
### "Error terms" formulation

$$\bf y = X\beta + \varepsilon$$
$$\varepsilon \sim \mathcal{N}(0,\sigma^2)$$

### "Distribution" formulation

$$\bf y \sim \mathcal{N}(\mu,\sigma^2)$$
$$\bf \mu = X\beta $$

---

We usually learn and conceive of the linear model in terms of "line of best fit" plus some "error" so the distribution formulation seems a bit weird at first.

. . .

\vfill

It is much more convenient when we start looking at extensions of linear regression, such as the linear mixed-effects model:

\begin{align*}
(\mathcal{Y} | \mathcal{B} = \bf{b} ) &\sim \mathcal{N}(\bf{ X\beta + Z b}, \sigma^2 \bf{I} ) \\
  \mathcal{B} &\sim \mathcal{N}(\bf{0}, \Sigma _\theta)
\end{align*}

## What do these parts all mean?

\begin{align*}
(\mathcal{Y} | \mathcal{B} = \bf{b} ) &\sim \mathcal{N}(\bf{\mu}, \sigma^2 \bf{I} ) \\
  \bf{\mu} &= \bf{ X\beta + Z b} \\
  \mathcal{B} &\sim \mathcal{N}(\bf{0}, \Sigma _\theta)
\end{align*}

- The reponse, conditional on the predicted mean $\mu$ is normally distributed
- $\sigma^2 \bf{I}$ homoskedacity in a multivariate context:
  - single shared residual variance ($\sigma^2$)
  - no autocorrelation or the like (everything happens on the diagonal $I$)
- The observed random effects $\bf{b}$ are draws from some latent distribution $\mathcal{B}$, whihc is normally distributed with mean 0 and some covariance structure $\Sigma _\theta$.
- Technically, we're not estimating the $\bf{b}$, but rather *predicting* them. Instead, we're estimating their covariance $\Sigma _\theta$.
- This is actually really profound and important because it means we don't add additional parameters to the model when adding e.g. additional participants, items, etc.
- More on this and its relationship to "shrinkage" in just a bit.

## Practice round: how does math match a model in R/Julia??

\begin{align*}
(\mathcal{Y} | \mathcal{B} = \bf{b} ) &\sim \mathcal{N}(\bf{\mu}, \sigma^2 \bf{I} ) \\
  \bf{\mu} &= \bf{ X\beta + Z b} \\
  \mathcal{B} &\sim \mathcal{N}(\bf{0}, \Sigma _\theta)
\end{align*}

```julia
Linear mixed model fit by maximum likelihood
 reaction ~ 1 + days + (1 + days | subj)
   logLik   -2 logLik     AIC       AICc        BIC
  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971

Variance components:
            Column    Variance Std.Dev.   Corr.
subj     (Intercept)  565.51067 23.78047
         days          32.68212  5.71683 +0.08
Residual              654.94145 25.59182
 Number of obs: 180; levels of grouping factors: 18

  Fixed-effects parameters:
──────────────────────────────────────────────────
                Coef.  Std. Error      z  Pr(>|z|)
──────────────────────────────────────────────────
(Intercept)  251.405      6.63226  37.91    <1e-99
days          10.4673     1.50224   6.97    <1e-11
──────────────────────────────────────────────────
```

## What does the term "random effect" refer to anyway?
Depending on context, it can refer to:
- the entire random variable \mathcal{B}$,
