---
title: Simulation in R
subtitle: Finally Getting to Actually Do Simulation
author: Phillip Alday
date: 6 May 2022
csl: apa.csl
lang: en-US
format: html
---

# From math to code

## Recall.....

$$Y \sim \mathcal{N}(\mu,\sigma^2)$$
$$\mu = X\beta $$

- tildes/distributions correspond to the `r*` family of functions (random draws)
- everything else is a little bit of matrix math

## Simple Linear Regression

```{r}
library("groundhog")
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10)  # NB: uniform
                  )

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

## Multiple Linear Regression

```{r}
set.seed(42)
form <- y ~ 1 + x * z
# intercept, x, z, x:z
beta <- c(1, 42, 27, 0.4)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),  # NB: uniform
                  z=runif(n, 25, 75))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
m2 <- lm(form, dat)
summary(m2)
```

## Not quite contrast coding, but the same problem

```{r}
set.seed(42)
form <- y ~ 1 + x * z
# intercept, x, z, x:z
beta <- c(1, 42, 27, 0.4)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),  # NB: uniform
                  z=runif(n, 25, 75))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
m2c <- lm(y ~ 1 + x * scale(z, center=TRUE, scale=FALSE), dat)
summary(m2c)
```


## Effects plots are your friend I

```{r}
groundhog.library("effects", "2022-04-28")
plot(allEffects(m2), multiline=TRUE, ci.style="band")
```

## Effects plots are your friend II

```{r}
plot(allEffects(m2c), multiline=TRUE, ci.style="band")
```

## Centering, scaling and contrast coding in linear regression

- Linear transformations of the numeric representation of your predictors changes the meaning and interpretation of your model coefficients
- Likewise, the way that categorical variables are changed into contrasts determines the meaning and interpretation of your model coefficients
- You cannot understand what a model means without knowing the choice of contrast and any transformation of numeric predictors! (see also Brehm and Alday, JML, in press)
- This is just as critical for simulation, if you want to simulate your hypotheses!

## Doing this in Julia

```{julia}
using DataFrames, GLM, StatsModels, Random

rng = MersenneTwister(42);
form = @formula(y ~ 1 + x * z);
# intercept, x, z, x&z
beta = [1, 42, 27, 0.4];
sig = 3.14;
n = 100;

# we can start with mean zero and then add mu in later
dat = DataFrame(y=sig * randn(rng, n), # multiply by sd to scale
                x=20 * rand(rng, n) .- 10,  # multiply by range and move left edge to correct postion
                z=50 * rand(rng, n) .+ 25);

# "hints" is the argument contrasts, etc.
X = modelmatrix(form, dat);


m2 = lm(form, dat)
```

# The Real World

## Imbalance

```{r}
groundhog.library("car", "2022-04-28")
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=sample(c("brown", "green", "orange"), n, replace=TRUE)  # NB: uniform
                  )
dat$x <- factor(dat$x)
contrasts(dat$x) <- contr.Sum(levels(dat$x))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

```{r}
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=sample(c("brown", "green", "orange"), n, replace=TRUE, prob=c(19/30, 1/30, 1/3))  # NB: uniform
                  )
dat$x <- factor(dat$x)
contrasts(dat$x) <- contr.Sum(levels(dat$x))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

## Heavy Tails

```{r}
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=sig * rt(n, df=3),
                  x=runif(n, -10, 10)  # NB: uniform
                  )

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

## Outliers

```{r}
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42)
sig <- 3.14
n <- 100

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10)  # NB: uniform
                  )

X <- model.matrix(form, dat)
# in 10% of data points, we add in a potentially huge second source of error
dat$y <- dat$y + rbinom(n, 1, prob=0.1) * rnorm(n, sd=100*sig)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

# Mixed Models

## Recall....

\begin{align*}
(Y | B = b ) &\sim \mathcal{N}( X\beta + Zb, \sigma^2 I ) \\
  B &\sim \mathcal{N}(0, \sigma _\theta)
\end{align*}

- tildes/distributions correspond to the `r*` family of functions (random draws)
- everything else is a little bit of matrix math

## A basic example

```{r}
set.seed(42)
groundhog.library("lme4", "2022-04-28")
form_fe <- y ~ 1 + x
form <- y ~ 1 + x + (1 | subj)

beta <- c(1, 42)
sig <- 1 # try 3.14
n <- 500 # try 100, 200, 500, 1000
n_subj <- 20 # try 10, 15, can't be more than 26
# was it better to increase n or n_subj?
subj_sd <- 0.5
b <- rnorm(n_subj, sd=subj_sd)

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),
                  subj=sample(LETTERS[1:n_subj], n, replace=TRUE))

X <- model.matrix(form_fe, dat)
Z <- model.matrix(~ 0 + subj, dat)

dat$y <- X %*% beta + Z %*% b + dat$y
# what happens if you use y ~ 1 + x + (1 + x | subj)?
mm1 <- lmer(form, dat, REML=FALSE)
summary(mm1)
```

## Check to make sure that we recover the BLUPs

```{r}
groundhog.library(c("broom.mixed","ggplot2"), "2022-04-28")
dd <- tidy(mm1, effects="ran_vals")
dd <- transform(dd, level=reorder(level,estimate))
truth <- data.frame(level=LETTERS[1:n_subj],estimate=b)
ggplot(dd,aes(x=level,y=estimate))+
    geom_pointrange(aes(ymin=estimate-2*std.error,
                        ymax=estimate+2*std.error)) + coord_flip() +
    geom_point(data=truth, colour="red") +
    theme_light()
```

## Complexity explodes when doing this by hand

```{r}
#| eval: false
set.seed(42)
form_fe <- y ~ 1 + x
form <- y ~ 1 + x + (1 + x| subj)

beta <- c(1, 42)
sig <- 1.414
n <- 1000
n_subj <- 26 # can't be more than 26 ;)
subj_sd <- c(0.5, 0.3)
# for now, we just let the RE correlations be zero
b <- c(rnorm(n_subj, sd=subj_sd[1]), rnorm(n_subj, sd=subj_sd[2]))

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),
                  subj=rep_len(LETTERS[1:n_subj], n))

X <- model.matrix(form_fe, dat)
Z_int <- model.matrix(~ 0 + subj, dat)
Z_slope <- ... # I can't be bothered ot actually do write this out....
Z <- cbind(Z_int, Z_slope)

dat$y <- X %*% beta + Z %*% b + dat$y
mm2 <- lmer(form, dat, REML=FALSE)
summary(mm1)
```

## Let lme4 do the hard work for you

```{r}
set.seed(42)
form <- y ~ 1 + x + (1 + x| subj)

beta <- c(1, 42)
sig <- 1.414
n <- 1000
n_subj <- 26 # can't be more than 26 ;)
# for now, we just let the RE correlations be zero
subj_sd <- c(0.5, 0.3)

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),
                  subj=rep_len(LETTERS[1:n_subj], n))
mm2 <- lmer(form, dat, REML=FALSE)
# this is garbage, but that's fine!
summary(mm2)
```

## simr fixef I

```{r}
groundhog.library("simr", "2022-04-28")
fixef(mm2) <- beta
# note that the estimates have been overridden, but nothing else updated
summary(mm2)
```

## simr fixef II

```{r}
summary(refit(mm2, simulate(mm2)[,1]))
```

## simr sig
```{r}
sigma(mm2) <- sig
summary(mm2)
```

## VarCorr: the variance-covariance of the random effects

```{r}
vc <- VarCorr(mm2)
print(vc)
```

## simr random effects I

```{r}
# this is on the variance/covariance scale
VarCorr(mm2) <- diag(subj_sd^2)
summary(mm2)
```

## simr random effects II

```{r}
# this is on the variance/covariance scale
sdcovar <- diag(subj_sd)
sdcovar[1,2] <- 0.1 # only need to fill in the upper triangle for simr
# sdcor2cov is an experimental function in lme4
VarCorr(mm2) <- sdcor2cov(sdcovar)
summary(mm2)
```

## simr also provides a way to do this directly....
```{r}
mm2alt <- makeLmer(form, beta, list(subject=sdcor2cov(sdcovar)), sig, dat)
```


## Did it work???
```{r}
dat$y <- simulate(mm2alt)[,1]
summary(lmer(form, dat, REML=FALSE))
```

## What about when we have more complex data?

- listeners (subjects) listened to speakers make either a positive or a negative statement
- each statement (item) appeared _either_ as a positive _or_ a negative statement
- we only had a small number of speakers, so modelling speaker-age or speaker-idiosyncraticities (by-speaker random effects) probably won't be meaningful or useful
- however, we had more than one speaker of each gender, so speaker-gender and speaker-idiosyncraticity isn't completely confounded
- we had a fairly large number of speakers
- we had a fairly large number of items
- how do speaker attributes, listener attributes, and the type of statement interact to influence listener response times in some task?

## Create the ground truth

```{r}
groundhog.library("tidyverse", "2022-04-28")
set.seed(42)

# ==== Generate the Design ===
# generate our sentences
n_sentences <- 60
sentences <- data.frame(sentence=sprintf("sent%02d",1:n_sentences),
                        # balanced: half positive, half negative
                        positive_statement=sample(c("yes", "no"), n_sentences,
                                                  replace=TRUE, prob=c(0.5, 0.5)))

# generate our speakers and determine which speakers spoke which sentences
n_spkrs <- 4
# all speakers produced all sentences
spkrs_sentences <- expand.grid(spkr=sprintf("spkr%02d",1:n_spkrs),
                               sentence=sprintf("sent%02d",1:n_sentences))

# speakers only produced half of all sentences
spkrs_sentences_1 <- expand.grid(spkr=sprintf("spkr%02d",1:(n_spkrs/2)),
                                 sentence=sprintf("sent%02d",1:(n_sentences/2)))
spkrs_sentences_2 <- expand.grid(spkr=sprintf("spkr%02d",(n_spkrs/2 + 1):n_spkrs),
                                 sentence=sprintf("sent%02d",(n_sentences/2 +1):n_sentences))
spkrs_sentences <- rbind(spkrs_sentences_1, spkrs_sentences_2)

spkrs <- data.frame(spkr=sprintf("spkr%02d",1:n_spkrs),
                    spkr_age=round(runif(n_spkrs,min=18,max=50)),
                    # keeping it simple for now
                    spkr_gender=sample(c("female", "male"), n_spkrs,
                                                  replace=TRUE, prob=c(0.5, 0.5)))
spkrs_sentences <- left_join(spkrs, spkrs_sentences, by="spkr")
stimuli <- left_join(spkrs_sentences, sentences, by="sentence")

# generate our subjects and match them to speakers
n_subjs <- 35 # odd number! need integer division (%/%) later
subjs <- data.frame(subj=sprintf("subj%02d",1:n_subjs),
                    subj_age=round(runif(n_subjs,min=18,max=28)),
                    # keeping it simple for now
                    subj_gender=sample(c("female", "male"), n_subjs,
                                              replace=TRUE, prob=c(0.5, 0.5)))
subjs_sentences_1 <- expand.grid(spkr=sprintf("spkr%02d",1:(n_spkrs/2)),
                                 subj=sprintf("subj%02d",1:(n_subjs %/% 2)))
subjs_sentences_2 <- expand.grid(spkr=sprintf("spkr%02d",(n_spkrs/2 + 1):n_spkrs),
                                 subj=sprintf("subj%02d",(n_subjs %/% 2 +1):n_subjs))
subjs_sentences <- rbind(subjs_sentences_1, subjs_sentences_2)

subjs_sentences <- left_join(subjs, subjs_sentences, by="subj")

experiment <- left_join(subjs_sentences, stimuli, by="spkr")

# === The Ground Truth Model ===
true_form <- rt ~ 1 + positive_statement * spkr_gender * subj_age * subj_gender +
             # listeners vary in their response to speaker attributes and statement polarity
            (1 + positive_statement * spkr_age * spkr_gender| subj) +
            # sentences have some idiosyncratic aspects that lead to different base RTs
            # but don't otherwise differ based on speaker/listener features
            (1 | sentence) # +
            # speakers vary in the RT they elicit for a particular polarity, e.g.,
            # by having different levels of associated affect/intonation
            # but with so few speakers, we can't even begin to model this
            # (1 + positive_statement | spkr)

# set our contrasts, because nothing make sense without it!
experiment$positive_statement <- factor(experiment$positive_statement)
contrasts(experiment$positive_statement) <- contr.Sum(levels(experiment$positive_statement))

experiment$subj_gender <- factor(experiment$subj_gender)
contrasts(experiment$subj_gender) <- contr.Sum(levels(experiment$subj_gender))

experiment$spkr_gender <- factor(experiment$spkr_gender)
contrasts(experiment$spkr_gender) <- contr.Sum(levels(experiment$spkr_gender))


# let's re-center our ages so that we're modelling the effect at age 25
# instead of at age 0, when we look at the slopes and the intercepts
experiment$subj_age <- experiment$subj_age - 25
experiment$spkr_age <- experiment$spkr_age - 25

# fixed effects, to get the coefficient names in the right order, use:
mmat <- model.matrix(~ 1 + positive_statement * spkr_gender * subj_age * subj_gender, data=experiment)
print(colnames(mmat)) # 16 coefficients!
# everything in milliseconds
beta <- c(300, # (Intercept) 300ms base RT
          +25, # positive_statement[S.no] 25ms slower RT than average for negative statements (=50ms slower than positive)
          +10, # spkr_gender[S.female] 10ms slower RT than average for female speakers
            5, # subj_age 5ms/year slower RT at age 26 (center+1) vs age 25 (center)
            7, # subj_gender[S.female] 7ms slower RT than average for female listeners
            3, # positive_statement[S.no]:spkr_gender[S.female]
            1, # positive_statement[S.no]:subj_age
            2, # spkr_gender[S.female]:subj_age
           -4, # positive_statement[S.no]:subj_gender[S.female]
            1, # spkr_gender[S.female]:subj_gender[S.female]
           -1, # subj_age:subj_gender[S.female]
            3, # positive_statement[S.no]:spkr_gender[S.female]:subj_age
           -2, # positive_statement[S.no]:spkr_gender[S.female]:subj_gender[S.female]
           -3, # positive_statement[S.no]:subj_age:subj_gender[S.female]
           -5, # spkr_gender[S.female]:subj_age:subj_gender[S.female]
          0.5 # positive_statement[S.no]:spkr_gender[S.female]:subj_age:subj_gender[S.female]
)
# residual standard deviation is 25ms
# this means that most trials are within Â±50ms of the mean
sig <- 25

# we don't need to define n explicitly -- this emerged naturally from our design!

# for now, we just let the RE correlations be zero because that makes our lives a lot
# easier and I have no idea what plausible values for the correlations are

# subject RE: (1 + positive_statement * spkr_age * spkr_gender| subj)
subj_mmat <- model.matrix(~ 1 + positive_statement * spkr_age * spkr_gender, experiment)
print(colnames(subj_mmat))
subj_sd <- c(20, # (Intercept)
             5, # positive_statement[S.no]
             1, # spkr_age
             2, # spkr_gender[S.female]
             1, # positive_statement[S.no]:spkr_age
             1, # positive_statement[S.no]:spkr_gender[S.female]
             1, # spkr_age:spkr_gender[S.female]
             1  # positive_statement[S.no]:spkr_age:spkr_gender[S.female])
)
subj_sd <- diag(subj_sd)

# sentence RE: (1|sentence)
sentence_sd <- as.matrix(5)

# speaker RE: (1 + positive_statement | spkr)
spkr_sd <- c(5, # (Intercept)
             3 # positive_statement[S.no]
)

spkr_sd <- diag(spkr_sd)

# there's a bug in simr -- if you get an error or things don't align,
# then try re-arranging the order so that things are ordered by decreasing number of levels:
# sentences (60), subjects (35), speakers (4)
# can't include speakers here because there are just too few to do anything meaningful
mm_sim <- makeLmer(true_form, beta,
                     list(sentence=sdcor2cov(sentence_sd),
                          subj=sdcor2cov(subj_sd)),
                          #spkr=sdcor2cov(spkr_sd)),
                     sig,
                     experiment)
# check to make sure things match up with your settings above
print(VarCorr(mm_sim))
print(fixef(mm_sim))
```

## Simulate some data

```{r}
# note if you change your design/setup, you can use the newdata argument to
# simulate with the model you have  instead of needing to use makeLmermod again
experiment$rt <- simulate(mm_sim)[, 1]
```

## Fitting the True Model

```{r}
mm_true <- lmer(true_form, data=experiment, REML=FALSE, control=lmerControl(calc.derivs=FALSE))
summary(mm_true)
```

## Fitting a Reduced Model

```{r}
form <- rt ~ 1 + positive_statement * spkr_gender * subj_age * subj_gender +
            (1 + positive_statement + spkr_age + spkr_gender| subj) +
            (1 | sentence)
mm_reduced <- lmer(form, data=experiment, REML=FALSE, control=lmerControl(calc.derivs=FALSE))
summary(mm_reduced)
```

## Even the reduced model is too complex for the amount of information we have!

```{r}
summary(rePCA(mm_reduced))
```

## Can't do it just once ....

- But doing things 1000x for each of a dozen different settings is a LOT of computation
- R has lots of nice features and it's gotten faster, but...
- Julia is faster
