---
title: Simulation and The Parametric Bootstrap
subtitle: Finally Getting to Actually Do Simulation
author: Phillip Alday
date: 6 May 2022
theme: Singapore
slide-level: 2
csl: apa.csl
lang: en-US
format: beamer
links-as-notes: true
---

# From math to code

## Recall.....

$$Y \sim \mathcal{N}(\mu,\sigma^2)$$
$$\mu = X\beta $$
- tildes/distributions correspond to the `r*` family of functions (random draws)
- everything else is a little bit of matrix math

## Simple Linear Regression {.shrink}

```{r}
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10)  # NB: uniform
                  )

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

## Multiple Linear Regression {.shrink}

```{r}
set.seed(42)
form <- y ~ 1 + x * z
# intercept, x, z, x:z
beta <- c(1, 42, 27, 0.4)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),  # NB: uniform
                  z=runif(n, 25, 75))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
m2 <- lm(form, dat)
summary(m2)
```

## Not quite contrast coding, but the same problem {.shrink}

```{r}
set.seed(42)
form <- y ~ 1 + x * z
# intercept, x, z, x:z
beta <- c(1, 42, 27, 0.4)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),  # NB: uniform
                  z=runif(n, 25, 75))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
m2c <- lm(y ~ 1 + x * scale(z, center=TRUE, scale=FALSE), dat)
summary(m2c)
```


## Effects plots are your friend I
```{r}
library(effects)
plot(allEffects(m2), multiline=TRUE, ci.style="band")
```

## Effects plots are your friend II
```{r}
plot(allEffects(m2), multiline=TRUE, ci.style="band")
```

## Centering, scaling and contrast coding in linear regression
- Linear transformations of the numeric representation of your predictors changes the meaning and interpretation of your model coefficients
- Likewise, the way that categorical variables are changed into contrasts determines the meaning and interpretation of your model coefficients
- You cannot understand what a model means without knowing the choice of contrast and any transformation of numeric predictors! (see also Brehm and Alday, JML, in press)
- This is just as critical for simulation, if you want to simulate your hypotheses!

## Doing this in Julia
```{julia}
using DataFrames, GLM, StatsModels, Random

rng = MersenneTwister(42);
form = @formula(y ~ 1 + x * z);
# intercept, x, z, x&z
beta = [1, 42, 27, 0.4];
sig = 3.14;
n = 100;

# we can start with mean zero and then add mu in later
dat = DataFrame(y=sig * randn(rng, n), # multiply by sd to scale
                x=20 * rand(rng, n) .- 10,  # multiply by range and move left edge to correct postion
                z=50 * rand(rng, n) .+ 25);

X = modelmatrix(form, dat); # "hints" is the argument contrasts, etc.

dat[!, :y] .=  X * beta .+ dat[!, :y];

m2 = lm(form, dat)
```

# The Real World

## Imbalance

```{r}
library("car")
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=sample(c("brown", "green", "orange"), n, replace=TRUE)  # NB: uniform
                  )
dat$x <- factor(dat$x)
contrasts(dat$x) <- contr.Sum(levels(dat$x))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

```{r}
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=rnorm(n, sd=sig),
                  x=sample(c("brown", "green", "orange"), n, replace=TRUE, prob=c(19/30, 1/30, 1/3))  # NB: uniform
                  )
dat$x <- factor(dat$x)
contrasts(dat$x) <- contr.Sum(levels(dat$x))

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

## Heavy Tails

```{r}
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42)
sig <- 3.14
n <- 100

# we can start with mean zero and then add mu in later
dat <- data.frame(y=sig * rt(n, df=3),
                  x=runif(n, -10, 10)  # NB: uniform
                  )

X <- model.matrix(form, dat)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

## Outliers

```{r}
set.seed(42)
form <- y ~ 1 + x
beta <- c(1, 42)
sig <- 3.14
n <- 100

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10)  # NB: uniform
                  )

X <- model.matrix(form, dat)
# in 10% of data points, we add in a potentially huge second source of error
dat$y <- dat$y + rbinom(n, 1, prob=0.1) * rnorm(n, sd=100*sig)

dat$y <- X %*% beta + dat$y
summary(lm(form, dat))
```

# Mixed Models

## Recall....

\begin{align*}
(Y | B = b ) &\sim \mathcal{N}( X\beta + Zb, \sigma^2 I ) \\
  B &\sim \mathcal{N}(0, \sigma _\theta)
\end{align*}

- tildes/distributions correspond to the `r*` family of functions (random draws)
- everything else is a little bit of matrix math

## A basic example
```{r}
set.seed(42)
library(lme4)
form_fe <- y ~ 1 + x
form <- y ~ 1 + x + (1 | subj)

beta <- c(1, 42)
sig <- 1 # try 3.14
n <- 500 # try 100, 200, 500, 1000
n_subj <- 20 # try 10, 15, can't be more than 26
# was it better to increase n or n_subj?
subj_sd <- 0.5
b <- rnorm(n_subj, sd=subj_sd)

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),
                  subj=sample(LETTERS[1:n_subj], n, replace=TRUE))

X <- model.matrix(form_fe, dat)
Z <- model.matrix(~ 0 + subj, dat)

dat$y <- X %*% beta + Z %*% b + dat$y
# what happens if you use y ~ 1 + x + (1 + x | subj)?
mm1 <- lmer(form, dat, REML=FALSE)
summary(mm1)
```

## Check to make sure that we recover the BLUPs

```{r}
library(broom.mixed)
library(ggplot2)
dd <- tidy(mm1, effects="ran_vals")
dd <- transform(dd, level=reorder(level,estimate))
truth <- data.frame(level=LETTERS[1:n_subj],estimate=b)
ggplot(dd,aes(x=level,y=estimate))+
    geom_pointrange(aes(ymin=estimate-2*std.error,
                        ymax=estimate+2*std.error)) + coord_flip() +
    geom_point(data=truth, colour="red") +
    theme_light()
```

## Complexity explodes when doing this by hand

```{r}
#| eval: false
set.seed(42)
form_fe <- y ~ 1 + x
form <- y ~ 1 + x + (1 + x| subj)

beta <- c(1, 42)
sig <- 1.414
n <- 1000
n_subj <- 26 # can't be more than 26 ;)
subj_sd <- c(0.5, 0.3)
# for now, we just let the RE correlations be zero
b <- c(rnorm(n_subj, sd=subj_sd[1]), rnorm(n_subj, sd=subj_sd[2]))

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),
                  subj=rep_len(LETTERS[1:n_subj], n))

X <- model.matrix(form_fe, dat)
Z_int <- model.matrix(~ 0 + subj, dat)
Z_slope <- ...
Z <- cbind(Z_int, Z_slope)

dat$y <- X %*% beta + Z %*% b + dat$y
mm2 <- lmer(form, dat, REML=FALSE)
summary(mm1)
```

## Let lme4 do the hard work for you

```{r}
set.seed(42)
form <- y ~ 1 + x + (1 + x| subj)

beta <- c(1, 42)
sig <- 1.414
n <- 1000
n_subj <- 26 # can't be more than 26 ;)
# for now, we just let the RE correlations be zero
subj_sd <- c(0.5, 0.3)

dat <- data.frame(y=rnorm(n, sd=sig),
                  x=runif(n, -10, 10),
                  subj=rep_len(LETTERS[1:n_subj], n))
mm2 <- lmer(form, dat, REML=FALSE)
# this is garbage, but that's fine!
summary(mm2)
```

## simr fixef I

```{r}
library("simr")
fixef(mm2) <- beta
# note that the estimates have been overridden, but nothing else updated
summary(mm2)
```

## simr fixef II

```{r}
summary(refit(mm2, simulate(mm2)[,1]))
```

## simr sig
```{r}
sigma(mm2) <- sig
summary(mm2)
```

## VarCorr: the variance-covariance of the random effects

```{r}
vc <- VarCorr(mm2)
print(vc)
```

## simr random effects I

```{r}
# this is on the variance/covariance scale
VarCorr(mm2) <- diag(subj_sd^2)
summary(mm2)
```

## simr random effects II

```{r}
# this is on the variance/covariance scale
sdcovar <- diag(subj_sd)
sdcovar[1,2] <- 0.1 # only need to fill in the upper triangle for simr
# sdcor2cov is an experimental function in lme4
VarCorr(mm2) <- sdcor2cov(sdcovar)
summary(mm2)
```

## simr also provides a way to do this directly....
```{r}
mm2alt <- makeLmer(form, beta, list(subject=sdcor2cov(sdcovar)), sig, dat)
```


## Did it work???
```{r}
dat$y <- simulate(mm2alt)[,1]
summary(lmer(form, dat, REML=FALSE))
```

## What about when we have more complex data?
<!-- joins -->

## Can't do it just once ....

# Julia

## How I learned to stop worrying and love the parametric bootstrap
